{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Convolution1D, MaxPooling1D, Flatten, Dense, Dropout, merge, concatenate\n",
    "#from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities import my_callbacks\n",
    "from utilities import data_helper\n",
    "import optparse\n",
    "import sys\n",
    "#from keras.callbacks import History\n",
    "#history = History()\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: None\n",
      "{'S': 52415, 'O': 30440, 'X': 162058, '-': 2449404}\n",
      "Total vocabulary size in the whole dataset: 4\n",
      "['-', 'O', 'S', 'X', '0']\n"
     ]
    }
   ],
   "source": [
    "vocab = data_helper.load_all(filelist=\"final_data/wsj.all\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading entity-gird for pos and neg documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading entity-gird for pos and neg documents...\")\n",
    "\n",
    "X_train_1, X_train_0, E = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.train\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_dev_1, X_dev_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.dev\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_test_1, X_test_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.test\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.03970985e-03,  -8.52192802e-03,   7.89863525e-03,\n",
       "         -1.27012905e-03,  -7.44644532e-03,   1.51715749e-03,\n",
       "          6.80941839e-03,  -1.29758898e-03,   3.91821121e-03,\n",
       "          3.69276194e-03,   4.01296731e-03,   5.59388526e-03,\n",
       "          2.85498737e-03,   9.22052336e-03,  -7.83070229e-03,\n",
       "          5.92212678e-03,   6.65160166e-03,  -4.67983274e-03,\n",
       "          6.73370778e-03,   6.42538131e-04,   3.38151282e-04,\n",
       "         -8.02824575e-03,   8.37737989e-03,   3.33316984e-03,\n",
       "         -6.50441043e-03,  -5.64616975e-03,  -6.42494388e-04,\n",
       "         -1.28217516e-03,   7.78708962e-03,  -5.54801458e-03,\n",
       "          1.78038732e-03,  -4.45596859e-03,   5.14443672e-04,\n",
       "         -4.81285778e-03,   5.78972608e-04,  -3.75718510e-03,\n",
       "          8.83244960e-04,  -5.15886991e-03,  -8.11523952e-03,\n",
       "         -6.21067241e-03,  -6.99429344e-03,   7.88893673e-03,\n",
       "         -3.98495804e-03,  -4.54271061e-03,  -9.87040510e-03,\n",
       "          1.96026909e-03,   5.88701763e-03,   1.97242136e-03,\n",
       "          2.29973385e-03,   7.40211540e-03,   4.58973377e-03,\n",
       "          5.30323564e-03,   9.62351956e-03,   4.27167651e-04,\n",
       "          6.96521619e-04,  -8.34030949e-03,  -9.61883544e-03,\n",
       "         -4.71642184e-03,   5.41444528e-03,   9.39280018e-03,\n",
       "         -3.70540600e-03,  -3.05309247e-05,  -5.93514391e-03,\n",
       "          3.68452820e-03,   6.74095603e-03,   5.47241431e-03,\n",
       "         -3.35617937e-03,  -7.20418892e-03,  -6.37036138e-03,\n",
       "          5.44302729e-03,  -7.49787227e-03,   6.22781819e-03,\n",
       "          3.98937542e-03,   3.85874427e-03,   3.19675998e-03,\n",
       "          8.77074575e-03,   6.91083613e-03,  -4.26424033e-03,\n",
       "          4.58911529e-03,  -1.83483112e-03,   4.05197539e-03,\n",
       "         -4.14700592e-03,   4.01784230e-03,  -8.17443463e-03,\n",
       "         -2.79983927e-03,  -8.28299137e-03,  -2.90348868e-04,\n",
       "         -5.07457570e-03,   3.52671527e-03,   6.48610863e-03,\n",
       "         -6.42917163e-03,  -9.60004403e-03,   4.66460833e-03,\n",
       "          3.63157190e-03,   5.98150329e-03,  -5.77202468e-03,\n",
       "          9.65175998e-03,  -9.37224661e-04,   2.83657238e-03,\n",
       "         -3.20497128e-03],\n",
       "       [  2.00556875e-03,   7.15154004e-03,  -9.26595988e-03,\n",
       "         -1.50933152e-03,   2.50299801e-03,   1.31978974e-03,\n",
       "         -1.93662433e-03,  -9.80060312e-03,   7.78215239e-03,\n",
       "          9.72938281e-03,  -5.11105870e-03,  -5.49461315e-03,\n",
       "         -3.22226401e-03,   1.23748419e-03,   6.53556715e-03,\n",
       "         -9.39870672e-03,   2.91323935e-03,   3.83534186e-04,\n",
       "         -5.89209335e-03,  -4.03723462e-03,   9.87739601e-03,\n",
       "          2.82453060e-03,  -1.90447570e-03,   3.71859121e-04,\n",
       "         -1.24896352e-03,  -8.13443100e-03,   7.17877344e-03,\n",
       "          5.52995466e-03,   3.86653472e-03,   3.33249517e-03,\n",
       "          7.54977901e-03,  -6.46473758e-03,  -8.06062570e-04,\n",
       "          8.90564097e-03,   5.57031435e-03,   6.97398455e-03,\n",
       "          8.64834344e-03,   8.65954556e-03,   5.34461954e-03,\n",
       "          1.90563048e-03,   7.00271892e-03,   3.68383557e-03,\n",
       "         -8.55548673e-03,  -9.85695253e-03,   5.74108943e-03,\n",
       "         -9.98522130e-03,  -8.87744654e-03,   9.53322159e-03,\n",
       "         -2.95036763e-03,   5.38258186e-03,  -1.73598470e-03,\n",
       "         -2.53614733e-03,   7.03238939e-03,  -9.51520318e-03,\n",
       "          6.04751343e-03,   3.82100858e-03,   8.72730990e-04,\n",
       "         -7.45246086e-04,  -9.80524659e-04,   8.26608914e-03,\n",
       "         -8.44378308e-03,   6.17030221e-03,   8.70886294e-04,\n",
       "         -3.56412801e-03,   7.43978740e-03,  -8.97574622e-03,\n",
       "         -3.53447596e-04,   2.17792942e-03,  -8.81936752e-03,\n",
       "          7.46201245e-03,   3.34172714e-03,  -3.40475083e-05,\n",
       "         -3.41394272e-03,   1.34714357e-03,   4.28490339e-03,\n",
       "         -9.17525658e-03,   8.96895257e-03,   7.67183111e-03,\n",
       "         -7.91059729e-03,   9.34368864e-03,  -8.89779433e-04,\n",
       "         -6.07066347e-03,  -1.34015708e-03,   9.54461738e-03,\n",
       "          4.37149251e-03,   9.51560836e-04,   9.06409623e-03,\n",
       "         -2.22718653e-03,  -1.89819991e-03,   6.49755717e-05,\n",
       "         -6.67397865e-03,  -1.69383535e-04,  -3.58314113e-03,\n",
       "          3.18389978e-03,  -9.08766452e-03,   1.93890453e-03,\n",
       "         -5.29422514e-03,  -5.55700597e-03,   1.17857158e-03,\n",
       "          5.08390446e-03],\n",
       "       [ -4.36874454e-04,   3.83542290e-03,   3.71072955e-03,\n",
       "          5.22901258e-03,   1.88307861e-03,   6.72930230e-03,\n",
       "         -9.54244126e-03,  -5.91427533e-03,   9.47509625e-03,\n",
       "         -7.60460420e-03,   8.35145585e-03,  -8.02860650e-03,\n",
       "         -1.94285362e-03,   7.03457216e-03,  -9.98844488e-03,\n",
       "         -3.64876849e-03,   6.40074084e-03,   1.76111963e-03,\n",
       "         -5.92580948e-03,  -6.33016956e-03,   5.03541606e-03,\n",
       "         -9.48613173e-05,   6.37712524e-03,   3.69759405e-03,\n",
       "          7.17927166e-03,   4.78988449e-03,  -6.78002244e-03,\n",
       "         -9.28028135e-03,  -8.20402398e-03,  -4.65543706e-03,\n",
       "          7.61669995e-03,   5.46350935e-03,   4.17880356e-03,\n",
       "          9.98240647e-04,   1.99592679e-03,  -3.34451226e-03,\n",
       "         -6.10425748e-03,   2.62612225e-03,  -4.29263697e-03,\n",
       "          7.36912052e-03,  -9.55013016e-03,  -7.02166502e-03,\n",
       "          3.27576755e-03,  -1.09433337e-03,   8.09281033e-03,\n",
       "         -2.84940696e-03,   7.76270843e-03,  -5.26429515e-03,\n",
       "          7.02096409e-03,   5.63213876e-03,   2.70713368e-03,\n",
       "         -5.98864457e-03,   2.55135943e-03,   9.10718245e-03,\n",
       "         -4.63783789e-03,  -9.48980781e-03,  -7.11079802e-03,\n",
       "          9.42105246e-03,  -7.13972885e-03,  -1.41241600e-03,\n",
       "          8.25268861e-03,  -1.60493889e-03,  -7.45583183e-03,\n",
       "          6.70053659e-03,  -7.40699320e-03,   3.66344219e-03,\n",
       "         -1.30044147e-03,  -9.19260644e-03,  -6.47929968e-03,\n",
       "          5.14934459e-03,   3.97271400e-03,  -9.14440440e-03,\n",
       "          4.23189016e-03,   9.00411635e-03,   2.48531256e-03,\n",
       "         -7.83476306e-03,  -3.01843927e-03,   7.24296047e-03,\n",
       "          6.84329966e-03,   6.94372125e-04,  -2.77253848e-04,\n",
       "          8.61697011e-03,   7.39013546e-03,   4.46831594e-03,\n",
       "          4.62836642e-03,  -5.34795681e-03,   4.06581359e-03,\n",
       "          3.36800403e-03,   6.48173620e-05,   1.50441259e-03,\n",
       "         -8.38663048e-03,  -9.65762347e-03,   5.28649006e-03,\n",
       "         -7.24742351e-03,  -5.85661551e-03,   5.22652483e-03,\n",
       "         -8.60296118e-03,  -8.32499360e-03,   4.13267142e-03,\n",
       "          1.41914328e-03],\n",
       "       [  6.93154481e-03,   9.16547606e-03,   1.99392883e-03,\n",
       "         -5.29819742e-04,  -4.09955426e-03,  -8.06082904e-03,\n",
       "         -9.77013081e-03,   6.44562312e-03,  -4.93103867e-03,\n",
       "          7.22110496e-03,  -2.87227721e-03,   9.90072912e-03,\n",
       "          8.73503079e-03,   5.81166238e-03,  -5.57627569e-03,\n",
       "          3.69763337e-03,   7.49217925e-03,  -7.17619524e-03,\n",
       "         -8.96205576e-03,  -8.00720073e-03,  -7.38672182e-03,\n",
       "          1.83291272e-03,   9.66638186e-03,  -6.79040205e-03,\n",
       "         -6.24846851e-04,  -4.24824988e-03,   2.48482096e-03,\n",
       "          1.82391988e-04,  -1.77711411e-03,   7.45908241e-03,\n",
       "          6.62311170e-03,  -3.34477536e-03,   9.18901991e-03,\n",
       "          5.51780708e-03,  -7.22178296e-03,  -5.28209322e-03,\n",
       "         -7.13861693e-03,   5.99682926e-03,   9.01329029e-03,\n",
       "          7.73432992e-03,  -6.85965738e-03,  -3.07227820e-03,\n",
       "         -6.57716265e-04,   2.84386441e-03,  -4.54389480e-03,\n",
       "          7.94174262e-03,   3.16383407e-03,  -9.13219041e-03,\n",
       "         -6.40885065e-03,  -2.58365987e-03,  -8.24858720e-03,\n",
       "         -3.06593419e-03,  -6.92022999e-03,   2.78720201e-03,\n",
       "          5.93874818e-03,  -6.45267272e-03,  -2.68662162e-03,\n",
       "          5.52163348e-03,  -5.99447981e-03,   5.84401076e-03,\n",
       "         -8.01333059e-03,  -5.98865377e-03,   6.73531036e-03,\n",
       "          8.22017676e-03,  -7.38520800e-03,  -4.99233287e-03,\n",
       "          5.02824003e-03,   7.90046691e-03,  -7.81945275e-03,\n",
       "         -1.87011938e-03,  -5.66210663e-03,   5.42003996e-03,\n",
       "          3.61492300e-03,  -7.78430869e-03,   9.72469108e-03,\n",
       "         -6.15450586e-03,   4.21170811e-03,  -2.97458497e-03,\n",
       "         -1.66842991e-03,   4.79884041e-03,  -6.07015569e-03,\n",
       "          5.23745753e-03,   8.68156626e-03,   8.92712300e-03,\n",
       "         -8.43770638e-03,  -5.64899390e-03,  -7.71097717e-03,\n",
       "         -8.96248102e-03,   9.25123095e-04,  -4.84230321e-03,\n",
       "         -4.69498413e-03,  -5.52693831e-03,   6.73851747e-03,\n",
       "         -4.97318859e-03,  -7.76573339e-03,   9.02336189e-03,\n",
       "         -3.44167929e-03,  -3.12420800e-03,  -6.65624008e-03,\n",
       "         -2.59779502e-03],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "Num of traing pairs: 23744\n",
      "Num of dev pairs: 2678\n",
      "Num of test pairs: 20411\n",
      ".....................................\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_train_1)\n",
    "num_dev   = len(X_dev_1)\n",
    "num_test  = len(X_test_1)\n",
    "#assign Y value\n",
    "y_train_1 = [1] * num_train \n",
    "y_dev_1 = [1] * num_dev \n",
    "y_test_1 = [1] * num_test \n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of dev pairs: \" + str(num_dev))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "#print(\"Num of permutation in train: \" + str(opts.p_num)) \n",
    "#print(\"The maximum in length for CNN: \" + str(opts.maxlen))\n",
    "print('.....................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the outputs\n",
    "y_train_1 = np_utils.to_categorical(y_train_1, 2)\n",
    "y_dev_1 = np_utils.to_categorical(y_dev_1, 2)\n",
    "y_test_1 = np_utils.to_categorical(y_test_1, 2)\n",
    "\n",
    "#randomly shuffle the training data\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_1)\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(113)\\n\\nE = 0.01 * np.random.uniform( -1.0, 1.0, (len(vocab), 100))\\nE[len(vocab)-1] = 0\\nE\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "np.random.seed(113)\n",
    "\n",
    "E = 0.01 * np.random.uniform( -1.0, 1.0, (len(vocab), 100))\n",
    "E[len(vocab)-1] = 0\n",
    "E\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(y_true, y_pred):\n",
    "    pos = y_pred[:, 0]\n",
    "    neg = y_pred[:, 1]\n",
    "    #loss = -K.sigmoid(pos-neg) # use \n",
    "    loss = K.maximum(1.0 + neg - pos, 0.0) #if you want to use margin ranking loss\n",
    "    return K.mean(loss) + 0 * y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define a CNN model for sequence of entities \n",
    "sent_input = Input(shape=(2000,), dtype='int32', name='sent_input')\n",
    "\n",
    "# embedding layer encodes the input into sequences of 300-dimenstional vectors. \n",
    "E = np.float32(E) #E was float64 which doesn't work for tensorflow conv1d function\n",
    "x = Embedding(input_dim=len(vocab), output_dim=100, weights= [E], input_length=2000)(sent_input)\n",
    "#print(x)\n",
    "\n",
    "# add a convolutiaon 1D layer\n",
    "#x = Dropout(dropout_ratio)(x)\n",
    "x = Convolution1D(filters=150, kernel_size=6, padding='valid', activation='relu')(x)\n",
    "#print(x)\n",
    "\n",
    "# add max pooling layers\n",
    "#x = AveragePooling1D(pool_length=pool_length)(x)\n",
    "x = MaxPooling1D(pool_size=6)(x)\n",
    "#print(x)\n",
    "\n",
    "#x = Dropout(opts.dropout_ratio)(x)\n",
    "x = Flatten()(x)\n",
    "#print(x)\n",
    "\n",
    "#x = Dense(hidden_size, activation='relu')(x)\n",
    "x = Dropout(0.5, seed = 100)(x)\n",
    "#print(x)\n",
    "\n",
    "# add latent coherence score\n",
    "out_x = Dense(1, activation=None)(x)\n",
    "#print(out_x)\n",
    "\n",
    "shared_cnn = Model(sent_input, out_x)\n",
    "#print(shared_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 100)         500       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1995, 150)         90150     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 332, 150)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 49800)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 49800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 49801     \n",
      "=================================================================\n",
      "Total params: 140,451\n",
      "Trainable params: 140,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(shared_cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if sys.path[0] == '':\n",
      "/home/tasnim/tensorflow/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "# Inputs of pos and neg document\n",
    "pos_input = Input(shape=(2000,), dtype='int32', name=\"pos_input\")\n",
    "neg_input = Input(shape=(2000,), dtype='int32', name=\"neg_input\")\n",
    "\n",
    "\n",
    "# these two models will share eveything from shared_cnn\n",
    "pos_branch = shared_cnn(pos_input)\n",
    "neg_branch = shared_cnn(neg_input)\n",
    "\n",
    "\n",
    "\n",
    "concatenated = merge([pos_branch, neg_branch], mode='concat', name=\"coherence_out\")\n",
    "#concatenated = concatenate([pos_branch, neg_branch], name=\"coherence_out\")\n",
    "# output is two latent coherence score\n",
    "\n",
    "\n",
    "final_model = Model([pos_input, neg_input], concatenated)\n",
    "\n",
    "\n",
    "#final_model.compile(loss='ranking_loss', optimizer='adam')\n",
    "final_model.compile(loss={'coherence_out': ranking_loss}, optimizer=\"rmsprop\")\n",
    "\n",
    "# setting callback\n",
    "histories = my_callbacks.Histories()\n",
    "\n",
    "#print(shared_cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1570e3a518>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit([X_train_1, X_train_0], y_train_1, validation_data=None, shuffle=False, epochs=1,\n",
    "                                  verbose=0, batch_size=32, callbacks=[histories])\n",
    "\n",
    "#print(history.history.keys())\n",
    "#print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :   1.00089\n",
      "Iteration  1 :   0.999261\n",
      "Iteration  2 :   0.998147\n",
      "Iteration  3 :   0.995858\n",
      "Iteration  4 :   0.994788\n",
      "Iteration  5 :   0.993319\n",
      "Iteration  6 :   0.982336\n",
      "Iteration  7 :   0.995565\n",
      "Iteration  8 :   0.993602\n",
      "Iteration  9 :   0.974734\n",
      "Iteration  10 :   0.968557\n",
      "Iteration  11 :   0.960383\n",
      "Iteration  12 :   0.981107\n",
      "Iteration  13 :   0.963892\n",
      "Iteration  14 :   0.920721\n",
      "Iteration  15 :   0.904191\n",
      "Iteration  16 :   0.924583\n",
      "Iteration  17 :   0.919438\n",
      "Iteration  18 :   0.900229\n",
      "Iteration  19 :   0.911221\n",
      "Iteration  20 :   0.834518\n",
      "Iteration  21 :   0.847019\n",
      "Iteration  22 :   0.802103\n",
      "Iteration  23 :   0.77268\n",
      "Iteration  24 :   0.712854\n",
      "Iteration  25 :   0.704299\n",
      "Iteration  26 :   0.790785\n",
      "Iteration  27 :   0.747592\n",
      "Iteration  28 :   0.700453\n",
      "Iteration  29 :   0.66265\n",
      "Iteration  30 :   0.574695\n",
      "Iteration  31 :   0.505085\n",
      "Iteration  32 :   0.519072\n",
      "Iteration  33 :   0.511181\n",
      "Iteration  34 :   0.536189\n",
      "Iteration  35 :   0.563653\n",
      "Iteration  36 :   0.660626\n",
      "Iteration  37 :   0.473921\n",
      "Iteration  38 :   0.479391\n",
      "Iteration  39 :   0.523092\n",
      "Iteration  40 :   0.3999\n",
      "Iteration  41 :   0.400841\n",
      "Iteration  42 :   0.482541\n",
      "Iteration  43 :   0.44443\n",
      "Iteration  44 :   0.480264\n",
      "Iteration  45 :   0.476399\n",
      "Iteration  46 :   0.411447\n",
      "Iteration  47 :   0.444576\n",
      "Iteration  48 :   0.349543\n",
      "Iteration  49 :   0.413588\n",
      "Iteration  50 :   0.531473\n",
      "Iteration  51 :   0.52651\n",
      "Iteration  52 :   0.453657\n",
      "Iteration  53 :   0.423529\n",
      "Iteration  54 :   0.649086\n",
      "Iteration  55 :   0.370694\n",
      "Iteration  56 :   0.478126\n",
      "Iteration  57 :   0.324527\n",
      "Iteration  58 :   0.409866\n",
      "Iteration  59 :   0.291544\n",
      "Iteration  60 :   0.304623\n",
      "Iteration  61 :   0.448353\n",
      "Iteration  62 :   0.49225\n",
      "Iteration  63 :   0.354585\n",
      "Iteration  64 :   0.364938\n",
      "Iteration  65 :   0.332116\n",
      "Iteration  66 :   0.372112\n",
      "Iteration  67 :   0.311109\n",
      "Iteration  68 :   0.302307\n",
      "Iteration  69 :   0.24614\n",
      "Iteration  70 :   0.468982\n",
      "Iteration  71 :   0.290322\n",
      "Iteration  72 :   0.374583\n",
      "Iteration  73 :   0.410318\n",
      "Iteration  74 :   0.401598\n",
      "Iteration  75 :   0.456111\n",
      "Iteration  76 :   0.701315\n",
      "Iteration  77 :   0.275768\n",
      "Iteration  78 :   0.396513\n",
      "Iteration  79 :   0.392699\n",
      "Iteration  80 :   0.371775\n",
      "Iteration  81 :   0.461607\n",
      "Iteration  82 :   0.220925\n",
      "Iteration  83 :   0.304844\n",
      "Iteration  84 :   0.246069\n",
      "Iteration  85 :   0.346863\n",
      "Iteration  86 :   0.214538\n",
      "Iteration  87 :   0.374132\n",
      "Iteration  88 :   0.530904\n",
      "Iteration  89 :   0.396672\n",
      "Iteration  90 :   0.482628\n",
      "Iteration  91 :   0.190319\n",
      "Iteration  92 :   0.182002\n",
      "Iteration  93 :   0.286716\n",
      "Iteration  94 :   0.284434\n",
      "Iteration  95 :   0.342087\n",
      "Iteration  96 :   0.177574\n",
      "Iteration  97 :   0.3139\n",
      "Iteration  98 :   0.309153\n",
      "Iteration  99 :   0.397414\n",
      "Iteration  100 :   0.37608\n",
      "Iteration  101 :   0.128587\n",
      "Iteration  102 :   0.17318\n",
      "Iteration  103 :   0.270382\n",
      "Iteration  104 :   0.499395\n",
      "Iteration  105 :   0.357641\n",
      "Iteration  106 :   0.3019\n",
      "Iteration  107 :   0.398122\n",
      "Iteration  108 :   0.271194\n",
      "Iteration  109 :   0.555281\n",
      "Iteration  110 :   0.56586\n",
      "Iteration  111 :   0.215181\n",
      "Iteration  112 :   0.373881\n",
      "Iteration  113 :   0.210326\n",
      "Iteration  114 :   0.399895\n",
      "Iteration  115 :   0.332582\n",
      "Iteration  116 :   0.265128\n",
      "Iteration  117 :   0.473785\n",
      "Iteration  118 :   0.332703\n",
      "Iteration  119 :   0.351518\n",
      "Iteration  120 :   0.313832\n",
      "Iteration  121 :   0.30738\n",
      "Iteration  122 :   0.279808\n",
      "Iteration  123 :   0.404781\n",
      "Iteration  124 :   0.420093\n",
      "Iteration  125 :   0.215547\n",
      "Iteration  126 :   0.252298\n",
      "Iteration  127 :   0.131958\n",
      "Iteration  128 :   0.157243\n",
      "Iteration  129 :   0.185753\n",
      "Iteration  130 :   0.497831\n",
      "Iteration  131 :   0.223655\n",
      "Iteration  132 :   0.192715\n",
      "Iteration  133 :   0.196536\n",
      "Iteration  134 :   0.304881\n",
      "Iteration  135 :   0.390329\n",
      "Iteration  136 :   0.142108\n",
      "Iteration  137 :   0.392294\n",
      "Iteration  138 :   0.171333\n",
      "Iteration  139 :   0.26928\n",
      "Iteration  140 :   0.168358\n",
      "Iteration  141 :   0.306945\n",
      "Iteration  142 :   0.236745\n",
      "Iteration  143 :   0.340924\n",
      "Iteration  144 :   0.476827\n",
      "Iteration  145 :   0.433732\n",
      "Iteration  146 :   0.408738\n",
      "Iteration  147 :   0.166237\n",
      "Iteration  148 :   0.410356\n",
      "Iteration  149 :   0.210241\n",
      "Iteration  150 :   0.235519\n",
      "Iteration  151 :   0.262945\n",
      "Iteration  152 :   0.344597\n",
      "Iteration  153 :   0.376642\n",
      "Iteration  154 :   0.390532\n",
      "Iteration  155 :   0.229043\n",
      "Iteration  156 :   0.0653265\n",
      "Iteration  157 :   0.258548\n",
      "Iteration  158 :   0.194694\n",
      "Iteration  159 :   0.295317\n",
      "Iteration  160 :   0.26301\n",
      "Iteration  161 :   0.330957\n",
      "Iteration  162 :   0.324872\n",
      "Iteration  163 :   0.316745\n",
      "Iteration  164 :   0.340556\n",
      "Iteration  165 :   0.114249\n",
      "Iteration  166 :   0.251248\n",
      "Iteration  167 :   0.387733\n",
      "Iteration  168 :   0.371784\n",
      "Iteration  169 :   0.306435\n",
      "Iteration  170 :   0.23741\n",
      "Iteration  171 :   0.184191\n",
      "Iteration  172 :   0.214964\n",
      "Iteration  173 :   0.259659\n",
      "Iteration  174 :   0.432129\n",
      "Iteration  175 :   0.164565\n",
      "Iteration  176 :   0.188808\n",
      "Iteration  177 :   0.312101\n",
      "Iteration  178 :   0.276407\n",
      "Iteration  179 :   0.287136\n",
      "Iteration  180 :   0.0861368\n",
      "Iteration  181 :   0.438487\n",
      "Iteration  182 :   0.134697\n",
      "Iteration  183 :   0.304765\n",
      "Iteration  184 :   0.198205\n",
      "Iteration  185 :   0.344966\n",
      "Iteration  186 :   0.303852\n",
      "Iteration  187 :   0.187729\n",
      "Iteration  188 :   0.0814936\n",
      "Iteration  189 :   0.193628\n",
      "Iteration  190 :   0.487773\n",
      "Iteration  191 :   0.167502\n",
      "Iteration  192 :   0.249558\n",
      "Iteration  193 :   0.223841\n",
      "Iteration  194 :   0.356487\n",
      "Iteration  195 :   0.0252657\n",
      "Iteration  196 :   0.294563\n",
      "Iteration  197 :   0.328947\n",
      "Iteration  198 :   0.290746\n",
      "Iteration  199 :   0.259025\n",
      "Iteration  200 :   0.0615058\n",
      "Iteration  201 :   0.269939\n",
      "Iteration  202 :   0.133008\n",
      "Iteration  203 :   0.253885\n",
      "Iteration  204 :   0.357104\n",
      "Iteration  205 :   0.154218\n",
      "Iteration  206 :   0.205573\n",
      "Iteration  207 :   0.309613\n",
      "Iteration  208 :   0.168464\n",
      "Iteration  209 :   0.221931\n",
      "Iteration  210 :   0.221241\n",
      "Iteration  211 :   0.287926\n",
      "Iteration  212 :   0.16789\n",
      "Iteration  213 :   0.2514\n",
      "Iteration  214 :   0.3024\n",
      "Iteration  215 :   0.280046\n",
      "Iteration  216 :   0.14718\n",
      "Iteration  217 :   0.149024\n",
      "Iteration  218 :   0.179071\n",
      "Iteration  219 :   0.373932\n",
      "Iteration  220 :   0.642605\n",
      "Iteration  221 :   0.258657\n",
      "Iteration  222 :   0.288743\n",
      "Iteration  223 :   0.109209\n",
      "Iteration  224 :   0.151176\n",
      "Iteration  225 :   0.299576\n",
      "Iteration  226 :   0.302569\n",
      "Iteration  227 :   0.361261\n",
      "Iteration  228 :   0.236208\n",
      "Iteration  229 :   0.171223\n",
      "Iteration  230 :   0.380846\n",
      "Iteration  231 :   0.1988\n",
      "Iteration  232 :   0.187382\n",
      "Iteration  233 :   0.0746708\n",
      "Iteration  234 :   0.105269\n",
      "Iteration  235 :   0.338048\n",
      "Iteration  236 :   0.302249\n",
      "Iteration  237 :   0.210137\n",
      "Iteration  238 :   0.309813\n",
      "Iteration  239 :   0.115148\n",
      "Iteration  240 :   0.266232\n",
      "Iteration  241 :   0.0842416\n",
      "Iteration  242 :   0.17472\n",
      "Iteration  243 :   0.261965\n",
      "Iteration  244 :   0.25774\n",
      "Iteration  245 :   0.42241\n",
      "Iteration  246 :   0.181452\n",
      "Iteration  247 :   0.2337\n",
      "Iteration  248 :   0.189102\n",
      "Iteration  249 :   0.292657\n",
      "Iteration  250 :   0.270661\n",
      "Iteration  251 :   0.0666444\n",
      "Iteration  252 :   0.17621\n",
      "Iteration  253 :   0.153734\n",
      "Iteration  254 :   0.14133\n",
      "Iteration  255 :   0.123873\n",
      "Iteration  256 :   0.15505\n",
      "Iteration  257 :   0.386\n",
      "Iteration  258 :   0.296854\n",
      "Iteration  259 :   0.0843236\n",
      "Iteration  260 :   0.26521\n",
      "Iteration  261 :   0.119428\n",
      "Iteration  262 :   0.205655\n",
      "Iteration  263 :   0.351072\n",
      "Iteration  264 :   0.139164\n",
      "Iteration  265 :   0.175769\n",
      "Iteration  266 :   0.250796\n",
      "Iteration  267 :   0.163308\n",
      "Iteration  268 :   0.191807\n",
      "Iteration  269 :   0.207701\n",
      "Iteration  270 :   0.33845\n",
      "Iteration  271 :   0.263539\n",
      "Iteration  272 :   0.352615\n",
      "Iteration  273 :   0.296924\n",
      "Iteration  274 :   0.211135\n",
      "Iteration  275 :   0.333623\n",
      "Iteration  276 :   0.308528\n",
      "Iteration  277 :   0.286213\n",
      "Iteration  278 :   0.177856\n",
      "Iteration  279 :   0.200169\n",
      "Iteration  280 :   0.355272\n",
      "Iteration  281 :   0.236762\n",
      "Iteration  282 :   0.246109\n",
      "Iteration  283 :   0.218053\n",
      "Iteration  284 :   0.128085\n",
      "Iteration  285 :   0.400571\n",
      "Iteration  286 :   0.246064\n",
      "Iteration  287 :   0.186143\n",
      "Iteration  288 :   0.264945\n",
      "Iteration  289 :   0.263056\n",
      "Iteration  290 :   0.299783\n",
      "Iteration  291 :   0.11266\n",
      "Iteration  292 :   0.198423\n",
      "Iteration  293 :   0.140692\n",
      "Iteration  294 :   0.271229\n",
      "Iteration  295 :   0.293574\n",
      "Iteration  296 :   0.144706\n",
      "Iteration  297 :   0.447272\n",
      "Iteration  298 :   0.122283\n",
      "Iteration  299 :   0.354649\n",
      "Iteration  300 :   0.103264\n",
      "Iteration  301 :   0.408761\n",
      "Iteration  302 :   0.0739505\n",
      "Iteration  303 :   0.451405\n",
      "Iteration  304 :   0.239805\n",
      "Iteration  305 :   0.406313\n",
      "Iteration  306 :   0.275498\n",
      "Iteration  307 :   0.232825\n",
      "Iteration  308 :   0.279456\n",
      "Iteration  309 :   0.206588\n",
      "Iteration  310 :   0.472389\n",
      "Iteration  311 :   0.210625\n",
      "Iteration  312 :   0.158346\n",
      "Iteration  313 :   0.0753241\n",
      "Iteration  314 :   0.357666\n",
      "Iteration  315 :   0.210693\n",
      "Iteration  316 :   0.242718\n",
      "Iteration  317 :   0.299983\n",
      "Iteration  318 :   0.123355\n",
      "Iteration  319 :   0.215748\n",
      "Iteration  320 :   0.224891\n",
      "Iteration  321 :   0.205664\n",
      "Iteration  322 :   0.148146\n",
      "Iteration  323 :   0.204817\n",
      "Iteration  324 :   0.338858\n",
      "Iteration  325 :   0.27876\n",
      "Iteration  326 :   0.127389\n",
      "Iteration  327 :   0.250181\n",
      "Iteration  328 :   0.0634823\n",
      "Iteration  329 :   0.409052\n",
      "Iteration  330 :   0.222544\n",
      "Iteration  331 :   0.31262\n",
      "Iteration  332 :   0.193975\n",
      "Iteration  333 :   0.0975111\n",
      "Iteration  334 :   0.129451\n",
      "Iteration  335 :   0.194194\n",
      "Iteration  336 :   0.261539\n",
      "Iteration  337 :   0.138121\n",
      "Iteration  338 :   0.237184\n",
      "Iteration  339 :   0.310035\n",
      "Iteration  340 :   0.223232\n",
      "Iteration  341 :   0.265048\n",
      "Iteration  342 :   0.280305\n",
      "Iteration  343 :   0.0779798\n",
      "Iteration  344 :   0.227884\n",
      "Iteration  345 :   0.209661\n",
      "Iteration  346 :   0.193866\n",
      "Iteration  347 :   0.354008\n",
      "Iteration  348 :   0.132002\n",
      "Iteration  349 :   0.138534\n",
      "Iteration  350 :   0.204519\n",
      "Iteration  351 :   0.068418\n",
      "Iteration  352 :   0.327603\n",
      "Iteration  353 :   0.257953\n",
      "Iteration  354 :   0.225554\n",
      "Iteration  355 :   0.146925\n",
      "Iteration  356 :   0.489577\n",
      "Iteration  357 :   0.327261\n",
      "Iteration  358 :   0.0421762\n",
      "Iteration  359 :   0.0674534\n",
      "Iteration  360 :   0.128796\n",
      "Iteration  361 :   0.248424\n",
      "Iteration  362 :   0.363269\n",
      "Iteration  363 :   0.051378\n",
      "Iteration  364 :   0.0423381\n",
      "Iteration  365 :   0.150423\n",
      "Iteration  366 :   0.366118\n",
      "Iteration  367 :   0.296637\n",
      "Iteration  368 :   0.168084\n",
      "Iteration  369 :   0.375448\n",
      "Iteration  370 :   0.1222\n",
      "Iteration  371 :   0.254491\n",
      "Iteration  372 :   0.172205\n",
      "Iteration  373 :   0.10772\n",
      "Iteration  374 :   0.171307\n",
      "Iteration  375 :   0.172621\n",
      "Iteration  376 :   0.341047\n",
      "Iteration  377 :   0.476675\n",
      "Iteration  378 :   0.00132465\n",
      "Iteration  379 :   0.18812\n",
      "Iteration  380 :   0.117396\n",
      "Iteration  381 :   0.175168\n",
      "Iteration  382 :   0.480368\n",
      "Iteration  383 :   0.0949829\n",
      "Iteration  384 :   0.297778\n",
      "Iteration  385 :   0.447478\n",
      "Iteration  386 :   0.076998\n",
      "Iteration  387 :   0.063494\n",
      "Iteration  388 :   0.139657\n",
      "Iteration  389 :   0.0782709\n",
      "Iteration  390 :   0.15888\n",
      "Iteration  391 :   0.0657389\n",
      "Iteration  392 :   0.295203\n",
      "Iteration  393 :   0.23545\n",
      "Iteration  394 :   0.154145\n",
      "Iteration  395 :   0.209867\n",
      "Iteration  396 :   0.274295\n",
      "Iteration  397 :   0.230112\n",
      "Iteration  398 :   0.178367\n",
      "Iteration  399 :   0.239702\n",
      "Iteration  400 :   0.198792\n",
      "Iteration  401 :   0.360485\n",
      "Iteration  402 :   0.137652\n",
      "Iteration  403 :   0.149699\n",
      "Iteration  404 :   0.0672085\n",
      "Iteration  405 :   0.142098\n",
      "Iteration  406 :   0.216445\n",
      "Iteration  407 :   0.291741\n",
      "Iteration  408 :   0.245584\n",
      "Iteration  409 :   0.16392\n",
      "Iteration  410 :   0.141403\n",
      "Iteration  411 :   0.0685678\n",
      "Iteration  412 :   0.294363\n",
      "Iteration  413 :   0.181136\n",
      "Iteration  414 :   0.0581086\n",
      "Iteration  415 :   0.102512\n",
      "Iteration  416 :   0.143389\n",
      "Iteration  417 :   0.120033\n",
      "Iteration  418 :   0.260427\n",
      "Iteration  419 :   0.419199\n",
      "Iteration  420 :   0.152681\n",
      "Iteration  421 :   0.147203\n",
      "Iteration  422 :   0.135072\n",
      "Iteration  423 :   0.184747\n",
      "Iteration  424 :   0.202994\n",
      "Iteration  425 :   0.0792627\n",
      "Iteration  426 :   0.252202\n",
      "Iteration  427 :   0.275074\n",
      "Iteration  428 :   0.14299\n",
      "Iteration  429 :   0.409483\n",
      "Iteration  430 :   0.238288\n",
      "Iteration  431 :   0.130048\n",
      "Iteration  432 :   0.131759\n",
      "Iteration  433 :   0.239157\n",
      "Iteration  434 :   0.06055\n",
      "Iteration  435 :   0.123006\n",
      "Iteration  436 :   0.235923\n",
      "Iteration  437 :   0.214514\n",
      "Iteration  438 :   0.374026\n",
      "Iteration  439 :   0.10669\n",
      "Iteration  440 :   0.0720582\n",
      "Iteration  441 :   0.192096\n",
      "Iteration  442 :   0.109231\n",
      "Iteration  443 :   0.208341\n",
      "Iteration  444 :   0.16813\n",
      "Iteration  445 :   0.157751\n",
      "Iteration  446 :   0.147345\n",
      "Iteration  447 :   0.394459\n",
      "Iteration  448 :   0.213055\n",
      "Iteration  449 :   0.112696\n",
      "Iteration  450 :   0.0977929\n",
      "Iteration  451 :   0.0742862\n",
      "Iteration  452 :   0.126274\n",
      "Iteration  453 :   0.0631139\n",
      "Iteration  454 :   0.203555\n",
      "Iteration  455 :   0.108443\n",
      "Iteration  456 :   0.408881\n",
      "Iteration  457 :   0.135325\n",
      "Iteration  458 :   0.197937\n",
      "Iteration  459 :   0.074964\n",
      "Iteration  460 :   0.333613\n",
      "Iteration  461 :   0.0869315\n",
      "Iteration  462 :   0.431513\n",
      "Iteration  463 :   0.0675051\n",
      "Iteration  464 :   0.137998\n",
      "Iteration  465 :   0.157157\n",
      "Iteration  466 :   0.155601\n",
      "Iteration  467 :   0.233425\n",
      "Iteration  468 :   0.174229\n",
      "Iteration  469 :   0.134546\n",
      "Iteration  470 :   0.365505\n",
      "Iteration  471 :   0.147094\n",
      "Iteration  472 :   0.144349\n",
      "Iteration  473 :   0.277531\n",
      "Iteration  474 :   0.243054\n",
      "Iteration  475 :   0.386178\n",
      "Iteration  476 :   0.229646\n",
      "Iteration  477 :   0.183603\n",
      "Iteration  478 :   0.160192\n",
      "Iteration  479 :   0.131553\n",
      "Iteration  480 :   0.362966\n",
      "Iteration  481 :   0.115177\n",
      "Iteration  482 :   0.139788\n",
      "Iteration  483 :   0.147408\n",
      "Iteration  484 :   0.0999577\n",
      "Iteration  485 :   0.211729\n",
      "Iteration  486 :   0.181067\n",
      "Iteration  487 :   0.253606\n",
      "Iteration  488 :   0.158246\n",
      "Iteration  489 :   0.164757\n",
      "Iteration  490 :   0.134747\n",
      "Iteration  491 :   0.309639\n",
      "Iteration  492 :   0.160089\n",
      "Iteration  493 :   0.185924\n",
      "Iteration  494 :   0.164639\n",
      "Iteration  495 :   0.152042\n",
      "Iteration  496 :   0.24761\n",
      "Iteration  497 :   0.241546\n",
      "Iteration  498 :   0.0804694\n",
      "Iteration  499 :   0.215123\n",
      "Iteration  500 :   0.0982509\n",
      "Iteration  501 :   0.112111\n",
      "Iteration  502 :   0.159388\n",
      "Iteration  503 :   0.0280533\n",
      "Iteration  504 :   0.195293\n",
      "Iteration  505 :   0.393432\n",
      "Iteration  506 :   0.19132\n",
      "Iteration  507 :   0.162848\n",
      "Iteration  508 :   0.313015\n",
      "Iteration  509 :   0.222657\n",
      "Iteration  510 :   0.110371\n",
      "Iteration  511 :   0.200202\n",
      "Iteration  512 :   0.157242\n",
      "Iteration  513 :   0.159186\n",
      "Iteration  514 :   0.104592\n",
      "Iteration  515 :   0.227582\n",
      "Iteration  516 :   0.0864935\n",
      "Iteration  517 :   0.264878\n",
      "Iteration  518 :   0.26228\n",
      "Iteration  519 :   0.202175\n",
      "Iteration  520 :   0.39304\n",
      "Iteration  521 :   0.287887\n",
      "Iteration  522 :   0.17628\n",
      "Iteration  523 :   0.163365\n",
      "Iteration  524 :   0.115828\n",
      "Iteration  525 :   0.131257\n",
      "Iteration  526 :   0.225828\n",
      "Iteration  527 :   0.147167\n",
      "Iteration  528 :   0.372414\n",
      "Iteration  529 :   0.111781\n",
      "Iteration  530 :   0.27328\n",
      "Iteration  531 :   0.196043\n",
      "Iteration  532 :   0.198339\n",
      "Iteration  533 :   0.150742\n",
      "Iteration  534 :   0.14426\n",
      "Iteration  535 :   0.13667\n",
      "Iteration  536 :   0.185496\n",
      "Iteration  537 :   0.0877647\n",
      "Iteration  538 :   0.0898175\n",
      "Iteration  539 :   0.280114\n",
      "Iteration  540 :   0.202028\n",
      "Iteration  541 :   0.228719\n",
      "Iteration  542 :   0.225927\n",
      "Iteration  543 :   0.142083\n",
      "Iteration  544 :   0.0950809\n",
      "Iteration  545 :   0.0540371\n",
      "Iteration  546 :   0.25844\n",
      "Iteration  547 :   0.269144\n",
      "Iteration  548 :   0.412252\n",
      "Iteration  549 :   0.0196664\n",
      "Iteration  550 :   0.11295\n",
      "Iteration  551 :   0.269241\n",
      "Iteration  552 :   0.0959067\n",
      "Iteration  553 :   0.178025\n",
      "Iteration  554 :   0.0755553\n",
      "Iteration  555 :   0.360918\n",
      "Iteration  556 :   0.342367\n",
      "Iteration  557 :   0.268013\n",
      "Iteration  558 :   0.103729\n",
      "Iteration  559 :   0.190889\n",
      "Iteration  560 :   0.0600884\n",
      "Iteration  561 :   0.163217\n",
      "Iteration  562 :   0.0993233\n",
      "Iteration  563 :   0.365185\n",
      "Iteration  564 :   0.0990415\n",
      "Iteration  565 :   0.152641\n",
      "Iteration  566 :   0.163554\n",
      "Iteration  567 :   0.381306\n",
      "Iteration  568 :   0.353467\n",
      "Iteration  569 :   0.226029\n",
      "Iteration  570 :   0.0976505\n",
      "Iteration  571 :   0.305337\n",
      "Iteration  572 :   0.320942\n",
      "Iteration  573 :   0.109587\n",
      "Iteration  574 :   0.0833397\n",
      "Iteration  575 :   0.158865\n",
      "Iteration  576 :   0.130858\n",
      "Iteration  577 :   0.0965667\n",
      "Iteration  578 :   0.219284\n",
      "Iteration  579 :   0.239298\n",
      "Iteration  580 :   0.216915\n",
      "Iteration  581 :   0.112242\n",
      "Iteration  582 :   0.0570073\n",
      "Iteration  583 :   0.208876\n",
      "Iteration  584 :   0.125356\n",
      "Iteration  585 :   0.422672\n",
      "Iteration  586 :   0.0813928\n",
      "Iteration  587 :   0.229896\n",
      "Iteration  588 :   0.0944743\n",
      "Iteration  589 :   0.177253\n",
      "Iteration  590 :   0.124107\n",
      "Iteration  591 :   0.183667\n",
      "Iteration  592 :   0.252017\n",
      "Iteration  593 :   0.282726\n",
      "Iteration  594 :   0.238959\n",
      "Iteration  595 :   0.119837\n",
      "Iteration  596 :   0.0430713\n",
      "Iteration  597 :   0.250057\n",
      "Iteration  598 :   0.121688\n",
      "Iteration  599 :   0.0283904\n",
      "Iteration  600 :   0.0794282\n",
      "Iteration  601 :   0.160831\n",
      "Iteration  602 :   0.0212946\n",
      "Iteration  603 :   0.274403\n",
      "Iteration  604 :   0.0638189\n",
      "Iteration  605 :   0.0959973\n",
      "Iteration  606 :   0.15226\n",
      "Iteration  607 :   0.233155\n",
      "Iteration  608 :   0.315403\n",
      "Iteration  609 :   0.346794\n",
      "Iteration  610 :   0.16134\n",
      "Iteration  611 :   0.0710053\n",
      "Iteration  612 :   0.114235\n",
      "Iteration  613 :   0.239389\n",
      "Iteration  614 :   0.217858\n",
      "Iteration  615 :   0.236443\n",
      "Iteration  616 :   0.166775\n",
      "Iteration  617 :   0.0233049\n",
      "Iteration  618 :   0.0268402\n",
      "Iteration  619 :   0.0409632\n",
      "Iteration  620 :   0.242403\n",
      "Iteration  621 :   0.126575\n",
      "Iteration  622 :   0.191208\n",
      "Iteration  623 :   0.125196\n",
      "Iteration  624 :   0.175732\n",
      "Iteration  625 :   0.112323\n",
      "Iteration  626 :   0.171378\n",
      "Iteration  627 :   0.0529447\n",
      "Iteration  628 :   0.0306158\n",
      "Iteration  629 :   0.321973\n",
      "Iteration  630 :   0.28842\n",
      "Iteration  631 :   0.188735\n",
      "Iteration  632 :   0.113603\n",
      "Iteration  633 :   0.220263\n",
      "Iteration  634 :   0.104903\n",
      "Iteration  635 :   0.141944\n",
      "Iteration  636 :   0.141728\n",
      "Iteration  637 :   0.117384\n",
      "Iteration  638 :   0.277122\n",
      "Iteration  639 :   0.171497\n",
      "Iteration  640 :   0.181707\n",
      "Iteration  641 :   0.258111\n",
      "Iteration  642 :   0.182441\n",
      "Iteration  643 :   0.25158\n",
      "Iteration  644 :   0.180333\n",
      "Iteration  645 :   0.264953\n",
      "Iteration  646 :   0.113971\n",
      "Iteration  647 :   0.130763\n",
      "Iteration  648 :   0.239935\n",
      "Iteration  649 :   0.242942\n",
      "Iteration  650 :   0.236791\n",
      "Iteration  651 :   0.117896\n",
      "Iteration  652 :   0.194741\n",
      "Iteration  653 :   0.263031\n",
      "Iteration  654 :   0.235306\n",
      "Iteration  655 :   0.0295081\n",
      "Iteration  656 :   0.148173\n",
      "Iteration  657 :   0.0821071\n",
      "Iteration  658 :   0.117264\n",
      "Iteration  659 :   0.147167\n",
      "Iteration  660 :   0.192547\n",
      "Iteration  661 :   0.0728343\n",
      "Iteration  662 :   0.11958\n",
      "Iteration  663 :   0.0573177\n",
      "Iteration  664 :   0.195912\n",
      "Iteration  665 :   0.0867591\n",
      "Iteration  666 :   0.259336\n",
      "Iteration  667 :   0.18573\n",
      "Iteration  668 :   0.281347\n",
      "Iteration  669 :   0.122619\n",
      "Iteration  670 :   0.199654\n",
      "Iteration  671 :   0.199851\n",
      "Iteration  672 :   0.168221\n",
      "Iteration  673 :   0.313933\n",
      "Iteration  674 :   0.0459814\n",
      "Iteration  675 :   0.164614\n",
      "Iteration  676 :   0.148349\n",
      "Iteration  677 :   0.0751357\n",
      "Iteration  678 :   0.114081\n",
      "Iteration  679 :   0.107222\n",
      "Iteration  680 :   0.0972576\n",
      "Iteration  681 :   0.247349\n",
      "Iteration  682 :   0.311885\n",
      "Iteration  683 :   0.231685\n",
      "Iteration  684 :   0.0598321\n",
      "Iteration  685 :   0.101116\n",
      "Iteration  686 :   0.121048\n",
      "Iteration  687 :   0.188849\n",
      "Iteration  688 :   0.136918\n",
      "Iteration  689 :   0.133571\n",
      "Iteration  690 :   0.178203\n",
      "Iteration  691 :   0.129526\n",
      "Iteration  692 :   0.323178\n",
      "Iteration  693 :   0.0665274\n",
      "Iteration  694 :   0.0913315\n",
      "Iteration  695 :   0.0480065\n",
      "Iteration  696 :   0.278654\n",
      "Iteration  697 :   0.256936\n",
      "Iteration  698 :   0.0759926\n",
      "Iteration  699 :   0.0751877\n",
      "Iteration  700 :   0.0845094\n",
      "Iteration  701 :   0.269991\n",
      "Iteration  702 :   0.09127\n",
      "Iteration  703 :   0.191385\n",
      "Iteration  704 :   0.0789342\n",
      "Iteration  705 :   0.168865\n",
      "Iteration  706 :   0.250217\n",
      "Iteration  707 :   0.0897284\n",
      "Iteration  708 :   0.0388923\n",
      "Iteration  709 :   0.143188\n",
      "Iteration  710 :   0.251201\n",
      "Iteration  711 :   0.00888443\n",
      "Iteration  712 :   0.218963\n",
      "Iteration  713 :   0.266754\n",
      "Iteration  714 :   0.129441\n",
      "Iteration  715 :   0.137877\n",
      "Iteration  716 :   0.238559\n",
      "Iteration  717 :   0.0906234\n",
      "Iteration  718 :   0.184571\n",
      "Iteration  719 :   0.130338\n",
      "Iteration  720 :   0.0210819\n",
      "Iteration  721 :   0.375563\n",
      "Iteration  722 :   0.196356\n",
      "Iteration  723 :   0.210532\n",
      "Iteration  724 :   0.182979\n",
      "Iteration  725 :   0.180367\n",
      "Iteration  726 :   0.044775\n",
      "Iteration  727 :   0.152184\n",
      "Iteration  728 :   0.516137\n",
      "Iteration  729 :   0.179538\n",
      "Iteration  730 :   0.118462\n",
      "Iteration  731 :   0.0835123\n",
      "Iteration  732 :   0.0695462\n",
      "Iteration  733 :   0.146515\n",
      "Iteration  734 :   0.169408\n",
      "Iteration  735 :   0.108801\n",
      "Iteration  736 :   0.231041\n",
      "Iteration  737 :   0.125971\n",
      "Iteration  738 :   0.265863\n",
      "Iteration  739 :   0.234879\n",
      "Iteration  740 :   0.174923\n",
      "Iteration  741 :   0.0992265\n"
     ]
    }
   ],
   "source": [
    "for (i, loss) in enumerate(histories.losses):\n",
    "    print(\"Iteration \",i, \":  \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
