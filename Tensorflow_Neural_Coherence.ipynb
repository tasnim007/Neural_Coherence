{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities import my_callbacks\n",
    "from utilities import data_helper\n",
    "import optparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: None\n",
      "{'X': 162058, 'O': 30440, '-': 2449404, 'S': 52415}\n",
      "Total vocabulary size in the whole dataset: 4\n",
      "['-', 'O', 'S', 'X', '0']\n"
     ]
    }
   ],
   "source": [
    "vocab = data_helper.load_all(filelist=\"final_data/wsj.all\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading entity-gird for pos and neg documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading entity-gird for pos and neg documents...\")\n",
    "\n",
    "X_train_1, X_train_0, E = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.train\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_dev_1, X_dev_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.dev\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_test_1, X_test_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.test\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.03970985e-03,  -8.52192802e-03,   7.89863525e-03,\n",
       "         -1.27012905e-03,  -7.44644532e-03,   1.51715749e-03,\n",
       "          6.80941839e-03,  -1.29758898e-03,   3.91821121e-03,\n",
       "          3.69276194e-03,   4.01296731e-03,   5.59388526e-03,\n",
       "          2.85498737e-03,   9.22052336e-03,  -7.83070229e-03,\n",
       "          5.92212678e-03,   6.65160166e-03,  -4.67983274e-03,\n",
       "          6.73370778e-03,   6.42538131e-04,   3.38151282e-04,\n",
       "         -8.02824575e-03,   8.37737989e-03,   3.33316984e-03,\n",
       "         -6.50441043e-03,  -5.64616975e-03,  -6.42494388e-04,\n",
       "         -1.28217516e-03,   7.78708962e-03,  -5.54801458e-03,\n",
       "          1.78038732e-03,  -4.45596859e-03,   5.14443672e-04,\n",
       "         -4.81285778e-03,   5.78972608e-04,  -3.75718510e-03,\n",
       "          8.83244960e-04,  -5.15886991e-03,  -8.11523952e-03,\n",
       "         -6.21067241e-03,  -6.99429344e-03,   7.88893673e-03,\n",
       "         -3.98495804e-03,  -4.54271061e-03,  -9.87040510e-03,\n",
       "          1.96026909e-03,   5.88701763e-03,   1.97242136e-03,\n",
       "          2.29973385e-03,   7.40211540e-03,   4.58973377e-03,\n",
       "          5.30323564e-03,   9.62351956e-03,   4.27167651e-04,\n",
       "          6.96521619e-04,  -8.34030949e-03,  -9.61883544e-03,\n",
       "         -4.71642184e-03,   5.41444528e-03,   9.39280018e-03,\n",
       "         -3.70540600e-03,  -3.05309247e-05,  -5.93514391e-03,\n",
       "          3.68452820e-03,   6.74095603e-03,   5.47241431e-03,\n",
       "         -3.35617937e-03,  -7.20418892e-03,  -6.37036138e-03,\n",
       "          5.44302729e-03,  -7.49787227e-03,   6.22781819e-03,\n",
       "          3.98937542e-03,   3.85874427e-03,   3.19675998e-03,\n",
       "          8.77074575e-03,   6.91083613e-03,  -4.26424033e-03,\n",
       "          4.58911529e-03,  -1.83483112e-03,   4.05197539e-03,\n",
       "         -4.14700592e-03,   4.01784230e-03,  -8.17443463e-03,\n",
       "         -2.79983927e-03,  -8.28299137e-03,  -2.90348868e-04,\n",
       "         -5.07457570e-03,   3.52671527e-03,   6.48610863e-03,\n",
       "         -6.42917163e-03,  -9.60004403e-03,   4.66460833e-03,\n",
       "          3.63157190e-03,   5.98150329e-03,  -5.77202468e-03,\n",
       "          9.65175998e-03,  -9.37224661e-04,   2.83657238e-03,\n",
       "         -3.20497128e-03],\n",
       "       [  2.00556875e-03,   7.15154004e-03,  -9.26595988e-03,\n",
       "         -1.50933152e-03,   2.50299801e-03,   1.31978974e-03,\n",
       "         -1.93662433e-03,  -9.80060312e-03,   7.78215239e-03,\n",
       "          9.72938281e-03,  -5.11105870e-03,  -5.49461315e-03,\n",
       "         -3.22226401e-03,   1.23748419e-03,   6.53556715e-03,\n",
       "         -9.39870672e-03,   2.91323935e-03,   3.83534186e-04,\n",
       "         -5.89209335e-03,  -4.03723462e-03,   9.87739601e-03,\n",
       "          2.82453060e-03,  -1.90447570e-03,   3.71859121e-04,\n",
       "         -1.24896352e-03,  -8.13443100e-03,   7.17877344e-03,\n",
       "          5.52995466e-03,   3.86653472e-03,   3.33249517e-03,\n",
       "          7.54977901e-03,  -6.46473758e-03,  -8.06062570e-04,\n",
       "          8.90564097e-03,   5.57031435e-03,   6.97398455e-03,\n",
       "          8.64834344e-03,   8.65954556e-03,   5.34461954e-03,\n",
       "          1.90563048e-03,   7.00271892e-03,   3.68383557e-03,\n",
       "         -8.55548673e-03,  -9.85695253e-03,   5.74108943e-03,\n",
       "         -9.98522130e-03,  -8.87744654e-03,   9.53322159e-03,\n",
       "         -2.95036763e-03,   5.38258186e-03,  -1.73598470e-03,\n",
       "         -2.53614733e-03,   7.03238939e-03,  -9.51520318e-03,\n",
       "          6.04751343e-03,   3.82100858e-03,   8.72730990e-04,\n",
       "         -7.45246086e-04,  -9.80524659e-04,   8.26608914e-03,\n",
       "         -8.44378308e-03,   6.17030221e-03,   8.70886294e-04,\n",
       "         -3.56412801e-03,   7.43978740e-03,  -8.97574622e-03,\n",
       "         -3.53447596e-04,   2.17792942e-03,  -8.81936752e-03,\n",
       "          7.46201245e-03,   3.34172714e-03,  -3.40475083e-05,\n",
       "         -3.41394272e-03,   1.34714357e-03,   4.28490339e-03,\n",
       "         -9.17525658e-03,   8.96895257e-03,   7.67183111e-03,\n",
       "         -7.91059729e-03,   9.34368864e-03,  -8.89779433e-04,\n",
       "         -6.07066347e-03,  -1.34015708e-03,   9.54461738e-03,\n",
       "          4.37149251e-03,   9.51560836e-04,   9.06409623e-03,\n",
       "         -2.22718653e-03,  -1.89819991e-03,   6.49755717e-05,\n",
       "         -6.67397865e-03,  -1.69383535e-04,  -3.58314113e-03,\n",
       "          3.18389978e-03,  -9.08766452e-03,   1.93890453e-03,\n",
       "         -5.29422514e-03,  -5.55700597e-03,   1.17857158e-03,\n",
       "          5.08390446e-03],\n",
       "       [ -4.36874454e-04,   3.83542290e-03,   3.71072955e-03,\n",
       "          5.22901258e-03,   1.88307861e-03,   6.72930230e-03,\n",
       "         -9.54244126e-03,  -5.91427533e-03,   9.47509625e-03,\n",
       "         -7.60460420e-03,   8.35145585e-03,  -8.02860650e-03,\n",
       "         -1.94285362e-03,   7.03457216e-03,  -9.98844488e-03,\n",
       "         -3.64876849e-03,   6.40074084e-03,   1.76111963e-03,\n",
       "         -5.92580948e-03,  -6.33016956e-03,   5.03541606e-03,\n",
       "         -9.48613173e-05,   6.37712524e-03,   3.69759405e-03,\n",
       "          7.17927166e-03,   4.78988449e-03,  -6.78002244e-03,\n",
       "         -9.28028135e-03,  -8.20402398e-03,  -4.65543706e-03,\n",
       "          7.61669995e-03,   5.46350935e-03,   4.17880356e-03,\n",
       "          9.98240647e-04,   1.99592679e-03,  -3.34451226e-03,\n",
       "         -6.10425748e-03,   2.62612225e-03,  -4.29263697e-03,\n",
       "          7.36912052e-03,  -9.55013016e-03,  -7.02166502e-03,\n",
       "          3.27576755e-03,  -1.09433337e-03,   8.09281033e-03,\n",
       "         -2.84940696e-03,   7.76270843e-03,  -5.26429515e-03,\n",
       "          7.02096409e-03,   5.63213876e-03,   2.70713368e-03,\n",
       "         -5.98864457e-03,   2.55135943e-03,   9.10718245e-03,\n",
       "         -4.63783789e-03,  -9.48980781e-03,  -7.11079802e-03,\n",
       "          9.42105246e-03,  -7.13972885e-03,  -1.41241600e-03,\n",
       "          8.25268861e-03,  -1.60493889e-03,  -7.45583183e-03,\n",
       "          6.70053659e-03,  -7.40699320e-03,   3.66344219e-03,\n",
       "         -1.30044147e-03,  -9.19260644e-03,  -6.47929968e-03,\n",
       "          5.14934459e-03,   3.97271400e-03,  -9.14440440e-03,\n",
       "          4.23189016e-03,   9.00411635e-03,   2.48531256e-03,\n",
       "         -7.83476306e-03,  -3.01843927e-03,   7.24296047e-03,\n",
       "          6.84329966e-03,   6.94372125e-04,  -2.77253848e-04,\n",
       "          8.61697011e-03,   7.39013546e-03,   4.46831594e-03,\n",
       "          4.62836642e-03,  -5.34795681e-03,   4.06581359e-03,\n",
       "          3.36800403e-03,   6.48173620e-05,   1.50441259e-03,\n",
       "         -8.38663048e-03,  -9.65762347e-03,   5.28649006e-03,\n",
       "         -7.24742351e-03,  -5.85661551e-03,   5.22652483e-03,\n",
       "         -8.60296118e-03,  -8.32499360e-03,   4.13267142e-03,\n",
       "          1.41914328e-03],\n",
       "       [  6.93154481e-03,   9.16547606e-03,   1.99392883e-03,\n",
       "         -5.29819742e-04,  -4.09955426e-03,  -8.06082904e-03,\n",
       "         -9.77013081e-03,   6.44562312e-03,  -4.93103867e-03,\n",
       "          7.22110496e-03,  -2.87227721e-03,   9.90072912e-03,\n",
       "          8.73503079e-03,   5.81166238e-03,  -5.57627569e-03,\n",
       "          3.69763337e-03,   7.49217925e-03,  -7.17619524e-03,\n",
       "         -8.96205576e-03,  -8.00720073e-03,  -7.38672182e-03,\n",
       "          1.83291272e-03,   9.66638186e-03,  -6.79040205e-03,\n",
       "         -6.24846851e-04,  -4.24824988e-03,   2.48482096e-03,\n",
       "          1.82391988e-04,  -1.77711411e-03,   7.45908241e-03,\n",
       "          6.62311170e-03,  -3.34477536e-03,   9.18901991e-03,\n",
       "          5.51780708e-03,  -7.22178296e-03,  -5.28209322e-03,\n",
       "         -7.13861693e-03,   5.99682926e-03,   9.01329029e-03,\n",
       "          7.73432992e-03,  -6.85965738e-03,  -3.07227820e-03,\n",
       "         -6.57716265e-04,   2.84386441e-03,  -4.54389480e-03,\n",
       "          7.94174262e-03,   3.16383407e-03,  -9.13219041e-03,\n",
       "         -6.40885065e-03,  -2.58365987e-03,  -8.24858720e-03,\n",
       "         -3.06593419e-03,  -6.92022999e-03,   2.78720201e-03,\n",
       "          5.93874818e-03,  -6.45267272e-03,  -2.68662162e-03,\n",
       "          5.52163348e-03,  -5.99447981e-03,   5.84401076e-03,\n",
       "         -8.01333059e-03,  -5.98865377e-03,   6.73531036e-03,\n",
       "          8.22017676e-03,  -7.38520800e-03,  -4.99233287e-03,\n",
       "          5.02824003e-03,   7.90046691e-03,  -7.81945275e-03,\n",
       "         -1.87011938e-03,  -5.66210663e-03,   5.42003996e-03,\n",
       "          3.61492300e-03,  -7.78430869e-03,   9.72469108e-03,\n",
       "         -6.15450586e-03,   4.21170811e-03,  -2.97458497e-03,\n",
       "         -1.66842991e-03,   4.79884041e-03,  -6.07015569e-03,\n",
       "          5.23745753e-03,   8.68156626e-03,   8.92712300e-03,\n",
       "         -8.43770638e-03,  -5.64899390e-03,  -7.71097717e-03,\n",
       "         -8.96248102e-03,   9.25123095e-04,  -4.84230321e-03,\n",
       "         -4.69498413e-03,  -5.52693831e-03,   6.73851747e-03,\n",
       "         -4.97318859e-03,  -7.76573339e-03,   9.02336189e-03,\n",
       "         -3.44167929e-03,  -3.12420800e-03,  -6.65624008e-03,\n",
       "         -2.59779502e-03],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "Num of traing pairs: 23744\n",
      "Num of dev pairs: 2678\n",
      "Num of test pairs: 20411\n",
      ".....................................\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_train_1)\n",
    "num_dev   = len(X_dev_1)\n",
    "num_test  = len(X_test_1)\n",
    "#assign Y value\n",
    "y_train_1 = [1] * num_train \n",
    "y_dev_1 = [1] * num_dev \n",
    "y_test_1 = [1] * num_test \n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of dev pairs: \" + str(num_dev))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "#print(\"Num of permutation in train: \" + str(opts.p_num)) \n",
    "#print(\"The maximum in length for CNN: \" + str(opts.maxlen))\n",
    "print('.....................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly shuffle the training data\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_1)\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X_positive, X_negative, vocab, E, print_ = False):\n",
    "    \"\"\"\n",
    "    Implements forward propagation of Neural coherence model\n",
    "    \n",
    "    Arguments:\n",
    "    X_positive -- A Placeholder for positive document\n",
    "    X_negative -- A Placeholder for negative document\n",
    "    vocab -- Vocabulary list of entire entity grid list\n",
    "    E -- initialized values for embedding matrix\n",
    "    print_ -- Whether size of the variables to be printed\n",
    "    \n",
    "    Returns: \n",
    "    out_positive: Coherence Score for positive document\n",
    "    out_negative: Coherence Score for negative document\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Placeholders\n",
    "    #X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    #X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    \n",
    "\n",
    "    ## First Layer of NN: Transform each grammatical role in the grid into distributed representation - a real valued vector\n",
    "    \n",
    "    \n",
    "    #Shared embedding matrix\n",
    "    #W_embedding = tf.get_variable(\"W_embedding\", [len(vocab), 100], initializer = tf.contrib.layers.xavier_initializer()) #embedding matrix \n",
    "    E = np.float32(E) # DataType of E is float64, which is not in list of allowed values in conv1D. Allowed DataType: float16, float32\n",
    "    W_embedding = tf.get_variable(\"W_embedding\", initializer = E) #embedding matrix \n",
    "   \n",
    "    \n",
    "    #Look up layer\n",
    "    \n",
    "    #for positive document\n",
    "    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    embedding_negative = tf.nn.embedding_lookup(W_embedding, X_negative)\n",
    "\n",
    "\n",
    "    ## Second Layer of NN: Convolution Layer\n",
    "    \n",
    "    \n",
    "    #shared filter and bias\n",
    "    w_size = 6       #filter_size\n",
    "    emb_size = 100   #embedding_size \n",
    "    nb_filter = 150  #num_filters \n",
    "\n",
    "    filter_shape = [w_size, emb_size, nb_filter]\n",
    "\n",
    "    #W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) #filter for covolution layer 1\n",
    "    W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 100)) #filter for covolution layer 1\n",
    "    b_conv_layer_1 =  tf.get_variable(\"b_conv_layer_1\", shape=[nb_filter], initializer = tf.constant_initializer(0.0))  #bias for convolution layer 1\n",
    "\n",
    "    \n",
    "       \n",
    "    #1D Convolution for positive document\n",
    "    conv_layer_1_positive = tf.nn.conv1d(embedding_positive, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_positive = tf.nn.bias_add(conv_layer_1_positive, b_conv_layer_1)    \n",
    "    h_conv_layer_1_positive = tf.nn.relu(conv_layer_1_with_bias_positive, name=\"relu_conv_layer_1_positive\") # Apply nonlinearity\n",
    "    \n",
    "    \n",
    "    #1D Convolution for negative document\n",
    "    conv_layer_1_negative = tf.nn.conv1d(embedding_negative, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_negative = tf.nn.bias_add(conv_layer_1_negative, b_conv_layer_1)    \n",
    "    h_conv_layer_1_negative = tf.nn.relu(conv_layer_1_with_bias_negative, name=\"relu_conv_layer_1_negative\") # Apply nonlinearity\n",
    "\n",
    "    \n",
    "\n",
    "    ## Third Layer of NN: Pooling Layer\n",
    "    \n",
    "    \n",
    "    #1D Pooling for positive document\n",
    "    m_layer_1_positive = tf.nn.pool(h_conv_layer_1_positive, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "    #1D Pooling for negative document\n",
    "    m_layer_1_negative = tf.nn.pool(h_conv_layer_1_negative, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Fourth Layer of NN: Fully Connected Layer\n",
    "    \n",
    "    #Dropout Early [As Dat Used]\n",
    "    \n",
    "    #for positive document\n",
    "    #drop_out_early_positive = tf.nn.dropout(m_layer_1_positive, keep_prob=0.5)\n",
    "    \n",
    "    #for negative document\n",
    "    #drop_out_early_negative = tf.nn.dropout(m_layer_1_negative, keep_prob=0.5)\n",
    "    \n",
    "    \n",
    "    #Flatten\n",
    "    \n",
    "    #for positive document\n",
    "    flatten_positive = tf.contrib.layers.flatten(m_layer_1_positive)\n",
    "    #flatten_positive = tf.contrib.layers.flatten(drop_out_early_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    flatten_negative = tf.contrib.layers.flatten(m_layer_1_negative)\n",
    "    #flatten_negative = tf.contrib.layers.flatten(drop_out_early_negative)\n",
    "    \n",
    "\n",
    "    #Dropout\n",
    "    \n",
    "    #for positive document\n",
    "    drop_out_positive = tf.nn.dropout(flatten_positive, keep_prob=0.5, seed=100)\n",
    "    \n",
    "    #for negative document\n",
    "    drop_out_negative = tf.nn.dropout(flatten_negative, keep_prob=0.5, seed=100)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Coherence Scoring\n",
    "    \n",
    "    #for positive document\n",
    "    out_positive = tf.contrib.layers.fully_connected(drop_out_positive, num_outputs = 1, activation_fn=None)\n",
    "    #out_positive = tf.sigmoid(out_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    out_negative = tf.contrib.layers.fully_connected(drop_out_negative, num_outputs = 1, activation_fn=None)\n",
    "    #out_negative = tf.sigmoid(out_negative)\n",
    "    \n",
    "    if(print_):\n",
    "        print(\"Layer (type)          Output Shape\")\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"\\nInputLayer:\")\n",
    "        print(\"X_positive           \",   X_positive.shape)\n",
    "        print(\"X_negative           \",   X_negative.shape)\n",
    "        print(\"\\nEmbedding Layer:\")\n",
    "        print(\"Embedding Matrix     \",   W_embedding.shape)\n",
    "        print(\"Embedding Positive   \",   embedding_positive.shape)\n",
    "        print(\"Embedding Negative   \",   embedding_negative.shape)\n",
    "        print(\"\\nConvolution 1D Layer:\")\n",
    "        print(\"Filter Shape         \",   W_conv_layer_1.shape)\n",
    "        print(\"Conv Positive        \",   h_conv_layer_1_positive.shape)\n",
    "        print(\"Conv Negative        \",   h_conv_layer_1_negative.shape)\n",
    "        print(\"\\nMax Pooling 1D Layer:\")\n",
    "        print(\"MaxPool Positive     \",   m_layer_1_positive.shape)\n",
    "        print(\"MaxPool Negative     \",   m_layer_1_negative.shape)\n",
    "        print(\"\\nFlatten Layer: \")\n",
    "        print(\"Flatten Positive     \",   flatten_positive.shape)\n",
    "        print(\"Flatten Negative     \",   flatten_negative.shape)\n",
    "        print(\"\\nDropout Layer: \")\n",
    "        print(\"Dropout Positive     \",   drop_out_positive.shape)\n",
    "        print(\"Dropout Negative     \",   drop_out_negative.shape)\n",
    "        print(\"\\nFully Connected Layer:\")\n",
    "        print(\"FC Positive          \",   out_positive.shape)\n",
    "        print(\"FC Negative          \",   out_negative.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return out_positive, out_negative\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(pos, neg):\n",
    "    \"\"\"\n",
    "    Implements the ranking objective.\n",
    "    \n",
    "    Arguments:\n",
    "    pos -- score for positive document batch\n",
    "    neg -- score for negative document batch\n",
    "    \n",
    "    Returns:\n",
    "    Average ranking loss for the batch  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.maximum(1.0 + neg - pos, 0.0) \n",
    "    #print(loss)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, m, mini_batch_size = 32):\n",
    "    \"\"\"\n",
    "    Creates minibatches.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Positive Documents\n",
    "    Y -- Negative Documents\n",
    "    m -- Number of Documents\n",
    "    mini_batch_size -- Size of each mini batch. \n",
    "    \n",
    "    Returns:\n",
    "    list of mini batches from the positive and negative documents.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    mini_batches = []\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)          Output Shape\n",
      "_________________________________________\n",
      "\n",
      "InputLayer:\n",
      "X_positive            (?, 2000)\n",
      "X_negative            (?, 2000)\n",
      "\n",
      "Embedding Layer:\n",
      "Embedding Matrix      (5, 100)\n",
      "Embedding Positive    (?, 2000, 100)\n",
      "Embedding Negative    (?, 2000, 100)\n",
      "\n",
      "Convolution 1D Layer:\n",
      "Filter Shape          (6, 100, 150)\n",
      "Conv Positive         (?, 1995, 150)\n",
      "Conv Negative         (?, 1995, 150)\n",
      "\n",
      "Max Pooling 1D Layer:\n",
      "MaxPool Positive      (?, 332, 150)\n",
      "MaxPool Negative      (?, 332, 150)\n",
      "\n",
      "Flatten Layer: \n",
      "Flatten Positive      (?, 49800)\n",
      "Flatten Negative      (?, 49800)\n",
      "\n",
      "Dropout Layer: \n",
      "Dropout Positive      (?, 49800)\n",
      "Dropout Negative      (?, 49800)\n",
      "\n",
      "Fully Connected Layer:\n",
      "FC Positive           (?, 1)\n",
      "FC Negative           (?, 1)\n"
     ]
    }
   ],
   "source": [
    "## Create Placeholders\n",
    "X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for positive document\n",
    "X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for negative document\n",
    "\n",
    "# Forward propagation\n",
    "score_positive, score_negative = forward_propagation(X_positive, X_negative, vocab, E, print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function:\n",
    "cost = ranking_loss(score_positive, score_negative)\n",
    "\n",
    "# Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, epsilon=1e-8).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :   1.00089\n",
      "Iteration  1 :   0.996619\n",
      "Iteration  2 :   0.99267\n",
      "Iteration  3 :   0.990049\n",
      "Iteration  4 :   0.98509\n",
      "Iteration  5 :   0.981425\n",
      "Iteration  6 :   0.977038\n",
      "Iteration  7 :   0.972658\n",
      "Iteration  8 :   0.967735\n",
      "Iteration  9 :   0.961783\n",
      "Iteration  10 :   0.957051\n",
      "Iteration  11 :   0.950258\n",
      "Iteration  12 :   0.94398\n",
      "Iteration  13 :   0.93509\n",
      "Iteration  14 :   0.92691\n",
      "Iteration  15 :   0.916664\n",
      "Iteration  16 :   0.905267\n",
      "Iteration  17 :   0.889641\n",
      "Iteration  18 :   0.87271\n",
      "Iteration  19 :   0.849821\n",
      "Iteration  20 :   0.822262\n",
      "Iteration  21 :   0.788423\n",
      "Iteration  22 :   0.743976\n",
      "Iteration  23 :   0.689716\n",
      "Iteration  24 :   0.613655\n",
      "Iteration  25 :   0.5239\n",
      "Iteration  26 :   0.401033\n",
      "Iteration  27 :   0.242984\n",
      "Iteration  28 :   0.0390434\n",
      "Iteration  29 :   0.0\n",
      "Iteration  30 :   0.0\n",
      "Iteration  31 :   0.0\n",
      "Iteration  32 :   0.0\n",
      "Iteration  33 :   0.0\n",
      "Iteration  34 :   0.0\n",
      "Iteration  35 :   0.0\n",
      "Iteration  36 :   0.0\n",
      "Iteration  37 :   0.0\n",
      "Iteration  38 :   0.0\n",
      "Iteration  39 :   0.0\n",
      "Iteration  40 :   0.0\n",
      "Iteration  41 :   0.0\n",
      "Iteration  42 :   0.0\n",
      "Iteration  43 :   0.0\n",
      "Iteration  44 :   0.0\n",
      "Iteration  45 :   0.0\n",
      "Iteration  46 :   0.0\n",
      "Iteration  47 :   0.0\n",
      "Iteration  48 :   0.0\n",
      "Iteration  49 :   0.0\n",
      "Iteration  50 :   0.0\n",
      "Iteration  51 :   0.0\n",
      "Iteration  52 :   0.0\n",
      "Iteration  53 :   0.0\n",
      "Iteration  54 :   0.0\n",
      "Iteration  55 :   0.0\n",
      "Iteration  56 :   0.0\n",
      "Iteration  57 :   0.0\n",
      "Iteration  58 :   0.0\n",
      "Iteration  59 :   0.0\n",
      "Iteration  60 :   0.0\n",
      "Iteration  61 :   0.0\n",
      "Iteration  62 :   0.0\n",
      "Iteration  63 :   0.0\n",
      "Iteration  64 :   0.0\n",
      "Iteration  65 :   0.0\n",
      "Iteration  66 :   0.0\n",
      "Iteration  67 :   0.0\n",
      "Iteration  68 :   0.0\n",
      "Iteration  69 :   0.0\n",
      "Iteration  70 :   0.0\n",
      "Iteration  71 :   0.0\n",
      "Iteration  72 :   0.0\n",
      "Iteration  73 :   0.0\n",
      "Iteration  74 :   0.0\n",
      "Iteration  75 :   0.0\n",
      "Iteration  76 :   0.0\n",
      "Iteration  77 :   0.0\n",
      "Iteration  78 :   0.0\n",
      "Iteration  79 :   0.0\n",
      "Iteration  80 :   0.0\n",
      "Iteration  81 :   0.0\n",
      "Iteration  82 :   0.0\n",
      "Iteration  83 :   0.0\n",
      "Iteration  84 :   0.0\n",
      "Iteration  85 :   0.0\n",
      "Iteration  86 :   0.0\n",
      "Iteration  87 :   0.0\n",
      "Iteration  88 :   0.0\n",
      "Iteration  89 :   0.0\n",
      "Iteration  90 :   0.0\n",
      "Iteration  91 :   0.0\n",
      "Iteration  92 :   0.0\n",
      "Iteration  93 :   0.0\n",
      "Iteration  94 :   0.0\n",
      "Iteration  95 :   0.0\n",
      "Iteration  96 :   0.0\n",
      "Iteration  97 :   0.0\n",
      "Iteration  98 :   0.0\n",
      "Iteration  99 :   0.0\n",
      "Iteration  100 :   0.0\n",
      "Iteration  101 :   0.0\n",
      "Iteration  102 :   0.0\n",
      "Iteration  103 :   0.0\n",
      "Iteration  104 :   0.0\n",
      "Iteration  105 :   0.0\n",
      "Iteration  106 :   0.0\n",
      "Iteration  107 :   0.0\n",
      "Iteration  108 :   0.0\n",
      "Iteration  109 :   0.0\n",
      "Iteration  110 :   0.0\n",
      "Iteration  111 :   0.0\n",
      "Iteration  112 :   0.0\n",
      "Iteration  113 :   0.0\n",
      "Iteration  114 :   0.0\n",
      "Iteration  115 :   0.0\n",
      "Iteration  116 :   0.0\n",
      "Iteration  117 :   0.0\n",
      "Iteration  118 :   0.0\n",
      "Iteration  119 :   0.0\n",
      "Iteration  120 :   0.0\n",
      "Iteration  121 :   0.0\n",
      "Iteration  122 :   0.0\n",
      "Iteration  123 :   0.0\n",
      "Iteration  124 :   0.0\n",
      "Iteration  125 :   0.0\n",
      "Iteration  126 :   0.0\n",
      "Iteration  127 :   0.0\n",
      "Iteration  128 :   0.0\n",
      "Iteration  129 :   0.0\n",
      "Iteration  130 :   0.0\n",
      "Iteration  131 :   0.0\n",
      "Iteration  132 :   0.0\n",
      "Iteration  133 :   0.0\n",
      "Iteration  134 :   0.0\n",
      "Iteration  135 :   0.0\n",
      "Iteration  136 :   0.0\n",
      "Iteration  137 :   0.0\n",
      "Iteration  138 :   0.0\n",
      "Iteration  139 :   0.0\n",
      "Iteration  140 :   0.0\n",
      "Iteration  141 :   0.0\n",
      "Iteration  142 :   0.0\n",
      "Iteration  143 :   0.0\n",
      "Iteration  144 :   0.0\n",
      "Iteration  145 :   0.0\n",
      "Iteration  146 :   0.0\n",
      "Iteration  147 :   0.0\n",
      "Iteration  148 :   0.0\n",
      "Iteration  149 :   0.0\n",
      "Iteration  150 :   0.0\n",
      "Iteration  151 :   0.0\n",
      "Iteration  152 :   0.0\n",
      "Iteration  153 :   0.0\n",
      "Iteration  154 :   0.0\n",
      "Iteration  155 :   0.0\n",
      "Iteration  156 :   0.0\n",
      "Iteration  157 :   0.0\n",
      "Iteration  158 :   0.0\n",
      "Iteration  159 :   0.0\n",
      "Iteration  160 :   0.0\n",
      "Iteration  161 :   0.0\n",
      "Iteration  162 :   0.0\n",
      "Iteration  163 :   0.0\n",
      "Iteration  164 :   0.0\n",
      "Iteration  165 :   0.0\n",
      "Iteration  166 :   0.0\n",
      "Iteration  167 :   0.0\n",
      "Iteration  168 :   0.0\n",
      "Iteration  169 :   0.0\n",
      "Iteration  170 :   0.0\n",
      "Iteration  171 :   0.0\n",
      "Iteration  172 :   0.0\n",
      "Iteration  173 :   0.0\n",
      "Iteration  174 :   0.0\n",
      "Iteration  175 :   0.0\n",
      "Iteration  176 :   0.0\n",
      "Iteration  177 :   0.0\n",
      "Iteration  178 :   0.0\n",
      "Iteration  179 :   0.0\n",
      "Iteration  180 :   0.0\n",
      "Iteration  181 :   0.0\n",
      "Iteration  182 :   0.0\n",
      "Iteration  183 :   0.0\n",
      "Iteration  184 :   0.0\n",
      "Iteration  185 :   0.0\n",
      "Iteration  186 :   0.0\n",
      "Iteration  187 :   0.0\n",
      "Iteration  188 :   0.0\n",
      "Iteration  189 :   0.0\n",
      "Iteration  190 :   0.0\n",
      "Iteration  191 :   0.0\n",
      "Iteration  192 :   0.0\n",
      "Iteration  193 :   0.0\n",
      "Iteration  194 :   0.0\n",
      "Iteration  195 :   0.0\n",
      "Iteration  196 :   0.0\n",
      "Iteration  197 :   0.0\n",
      "Iteration  198 :   0.0\n",
      "Iteration  199 :   0.0\n",
      "Iteration  200 :   0.0\n",
      "Iteration  201 :   0.0\n",
      "Iteration  202 :   0.0\n",
      "Iteration  203 :   0.0\n",
      "Iteration  204 :   0.0\n",
      "Iteration  205 :   0.0\n",
      "Iteration  206 :   0.0\n",
      "Iteration  207 :   0.0\n",
      "Iteration  208 :   0.0\n",
      "Iteration  209 :   0.0\n",
      "Iteration  210 :   0.0\n",
      "Iteration  211 :   0.0\n",
      "Iteration  212 :   0.0\n",
      "Iteration  213 :   0.0\n",
      "Iteration  214 :   0.0\n",
      "Iteration  215 :   0.0\n",
      "Iteration  216 :   0.0\n",
      "Iteration  217 :   0.0\n",
      "Iteration  218 :   0.0\n",
      "Iteration  219 :   0.0\n",
      "Iteration  220 :   0.0\n",
      "Iteration  221 :   0.0\n",
      "Iteration  222 :   0.0\n",
      "Iteration  223 :   0.0\n",
      "Iteration  224 :   0.0\n",
      "Iteration  225 :   0.0\n",
      "Iteration  226 :   0.0\n",
      "Iteration  227 :   0.0\n",
      "Iteration  228 :   0.0\n",
      "Iteration  229 :   0.0\n",
      "Iteration  230 :   0.0\n",
      "Iteration  231 :   0.0\n",
      "Iteration  232 :   0.0\n",
      "Iteration  233 :   0.0\n",
      "Iteration  234 :   0.0\n",
      "Iteration  235 :   0.0\n",
      "Iteration  236 :   0.0\n",
      "Iteration  237 :   0.0\n",
      "Iteration  238 :   0.0\n",
      "Iteration  239 :   0.0\n",
      "Iteration  240 :   0.0\n",
      "Iteration  241 :   0.0\n",
      "Iteration  242 :   0.0\n",
      "Iteration  243 :   0.0\n",
      "Iteration  244 :   0.0\n",
      "Iteration  245 :   0.0\n",
      "Iteration  246 :   0.0\n",
      "Iteration  247 :   0.0\n",
      "Iteration  248 :   0.0\n",
      "Iteration  249 :   0.0\n",
      "Iteration  250 :   0.0\n",
      "Iteration  251 :   0.0\n",
      "Iteration  252 :   0.0\n",
      "Iteration  253 :   0.0\n",
      "Iteration  254 :   0.0\n",
      "Iteration  255 :   0.0\n",
      "Iteration  256 :   0.0\n",
      "Iteration  257 :   0.0\n",
      "Iteration  258 :   0.0\n",
      "Iteration  259 :   0.0\n",
      "Iteration  260 :   0.0\n",
      "Iteration  261 :   0.0\n",
      "Iteration  262 :   0.0\n",
      "Iteration  263 :   0.0\n",
      "Iteration  264 :   0.0\n",
      "Iteration  265 :   0.0\n",
      "Iteration  266 :   0.0\n",
      "Iteration  267 :   0.0\n",
      "Iteration  268 :   0.0\n",
      "Iteration  269 :   0.0\n",
      "Iteration  270 :   0.0\n",
      "Iteration  271 :   0.0\n",
      "Iteration  272 :   0.0\n",
      "Iteration  273 :   0.0\n",
      "Iteration  274 :   0.0\n",
      "Iteration  275 :   0.0\n",
      "Iteration  276 :   0.0\n",
      "Iteration  277 :   0.0\n",
      "Iteration  278 :   0.0\n",
      "Iteration  279 :   0.0\n",
      "Iteration  280 :   0.0\n",
      "Iteration  281 :   0.0\n",
      "Iteration  282 :   0.0\n",
      "Iteration  283 :   0.0\n",
      "Iteration  284 :   0.0\n",
      "Iteration  285 :   0.0\n",
      "Iteration  286 :   0.0\n",
      "Iteration  287 :   0.0\n",
      "Iteration  288 :   0.0\n",
      "Iteration  289 :   0.0\n",
      "Iteration  290 :   0.0\n",
      "Iteration  291 :   0.0\n",
      "Iteration  292 :   0.0\n",
      "Iteration  293 :   0.0\n",
      "Iteration  294 :   0.0\n",
      "Iteration  295 :   0.0\n",
      "Iteration  296 :   0.0\n",
      "Iteration  297 :   0.0\n",
      "Iteration  298 :   0.0\n",
      "Iteration  299 :   0.0\n",
      "Iteration  300 :   0.0\n",
      "Iteration  301 :   0.0\n",
      "Iteration  302 :   0.0\n",
      "Iteration  303 :   0.0\n",
      "Iteration  304 :   0.0\n",
      "Iteration  305 :   0.0\n",
      "Iteration  306 :   0.0\n",
      "Iteration  307 :   0.0\n",
      "Iteration  308 :   0.0\n",
      "Iteration  309 :   0.0\n",
      "Iteration  310 :   0.0\n",
      "Iteration  311 :   0.0\n",
      "Iteration  312 :   0.0\n",
      "Iteration  313 :   0.0\n",
      "Iteration  314 :   0.0\n",
      "Iteration  315 :   0.0\n",
      "Iteration  316 :   0.0\n",
      "Iteration  317 :   0.0\n",
      "Iteration  318 :   0.0\n",
      "Iteration  319 :   0.0\n",
      "Iteration  320 :   0.0\n",
      "Iteration  321 :   0.0\n",
      "Iteration  322 :   0.0\n",
      "Iteration  323 :   0.0\n",
      "Iteration  324 :   0.0\n",
      "Iteration  325 :   0.0\n",
      "Iteration  326 :   0.0\n",
      "Iteration  327 :   0.0\n",
      "Iteration  328 :   0.0\n",
      "Iteration  329 :   0.0\n",
      "Iteration  330 :   0.0\n",
      "Iteration  331 :   0.0\n",
      "Iteration  332 :   0.0\n",
      "Iteration  333 :   0.0\n",
      "Iteration  334 :   0.0\n",
      "Iteration  335 :   0.0\n",
      "Iteration  336 :   0.0\n",
      "Iteration  337 :   0.0\n",
      "Iteration  338 :   0.0\n",
      "Iteration  339 :   0.0\n",
      "Iteration  340 :   0.0\n",
      "Iteration  341 :   0.0\n",
      "Iteration  342 :   0.0\n",
      "Iteration  343 :   0.0\n",
      "Iteration  344 :   0.0\n",
      "Iteration  345 :   0.0\n",
      "Iteration  346 :   0.0\n",
      "Iteration  347 :   0.0\n",
      "Iteration  348 :   0.0\n",
      "Iteration  349 :   0.0\n",
      "Iteration  350 :   0.0\n",
      "Iteration  351 :   0.0\n",
      "Iteration  352 :   0.0\n",
      "Iteration  353 :   0.0\n",
      "Iteration  354 :   0.0\n",
      "Iteration  355 :   0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  356 :   0.0\n",
      "Iteration  357 :   0.0\n",
      "Iteration  358 :   0.0\n",
      "Iteration  359 :   0.0\n",
      "Iteration  360 :   0.0\n",
      "Iteration  361 :   0.0\n",
      "Iteration  362 :   0.0\n",
      "Iteration  363 :   0.0\n",
      "Iteration  364 :   0.0\n",
      "Iteration  365 :   0.0\n",
      "Iteration  366 :   0.0\n",
      "Iteration  367 :   0.0\n",
      "Iteration  368 :   0.0\n",
      "Iteration  369 :   0.0\n",
      "Iteration  370 :   0.0\n",
      "Iteration  371 :   0.0\n",
      "Iteration  372 :   0.0\n",
      "Iteration  373 :   0.0\n",
      "Iteration  374 :   0.0\n",
      "Iteration  375 :   0.0\n",
      "Iteration  376 :   0.0\n",
      "Iteration  377 :   0.0\n",
      "Iteration  378 :   0.0\n",
      "Iteration  379 :   0.0\n",
      "Iteration  380 :   0.0\n",
      "Iteration  381 :   0.0\n",
      "Iteration  382 :   0.0\n",
      "Iteration  383 :   0.0\n",
      "Iteration  384 :   0.0\n",
      "Iteration  385 :   0.0\n",
      "Iteration  386 :   0.0\n",
      "Iteration  387 :   0.0\n",
      "Iteration  388 :   0.0\n",
      "Iteration  389 :   0.0\n",
      "Iteration  390 :   0.0\n",
      "Iteration  391 :   0.0\n",
      "Iteration  392 :   0.0\n",
      "Iteration  393 :   0.0\n",
      "Iteration  394 :   0.0\n",
      "Iteration  395 :   0.0\n",
      "Iteration  396 :   0.0\n",
      "Iteration  397 :   0.0\n",
      "Iteration  398 :   0.0\n",
      "Iteration  399 :   0.0\n",
      "Iteration  400 :   0.0\n",
      "Iteration  401 :   0.0\n",
      "Iteration  402 :   0.0\n",
      "Iteration  403 :   0.0\n",
      "Iteration  404 :   0.0\n",
      "Iteration  405 :   0.0\n",
      "Iteration  406 :   0.0\n",
      "Iteration  407 :   0.0\n",
      "Iteration  408 :   0.0\n",
      "Iteration  409 :   0.0\n",
      "Iteration  410 :   0.0\n",
      "Iteration  411 :   0.0\n",
      "Iteration  412 :   0.0\n",
      "Iteration  413 :   0.0\n",
      "Iteration  414 :   0.0\n",
      "Iteration  415 :   0.0\n",
      "Iteration  416 :   0.0\n",
      "Iteration  417 :   0.0\n",
      "Iteration  418 :   0.0\n",
      "Iteration  419 :   0.0\n",
      "Iteration  420 :   0.0\n",
      "Iteration  421 :   0.0\n",
      "Iteration  422 :   0.0\n",
      "Iteration  423 :   0.0\n",
      "Iteration  424 :   0.0\n",
      "Iteration  425 :   0.0\n",
      "Iteration  426 :   0.0\n",
      "Iteration  427 :   0.0\n",
      "Iteration  428 :   0.0\n",
      "Iteration  429 :   0.0\n",
      "Iteration  430 :   0.0\n",
      "Iteration  431 :   0.0\n",
      "Iteration  432 :   0.0\n",
      "Iteration  433 :   0.0\n",
      "Iteration  434 :   0.0\n",
      "Iteration  435 :   0.0\n",
      "Iteration  436 :   0.0\n",
      "Iteration  437 :   0.0\n",
      "Iteration  438 :   0.0\n",
      "Iteration  439 :   0.0\n",
      "Iteration  440 :   0.0\n",
      "Iteration  441 :   0.0\n",
      "Iteration  442 :   0.0\n",
      "Iteration  443 :   0.0\n",
      "Iteration  444 :   0.0\n",
      "Iteration  445 :   0.0\n",
      "Iteration  446 :   0.0\n",
      "Iteration  447 :   0.0\n",
      "Iteration  448 :   0.0\n",
      "Iteration  449 :   0.0\n",
      "Iteration  450 :   0.0\n",
      "Iteration  451 :   0.0\n",
      "Iteration  452 :   0.0\n",
      "Iteration  453 :   0.0\n",
      "Iteration  454 :   0.0\n",
      "Iteration  455 :   0.0\n",
      "Iteration  456 :   0.0\n",
      "Iteration  457 :   0.0\n",
      "Iteration  458 :   0.0\n",
      "Iteration  459 :   0.0\n",
      "Iteration  460 :   0.0\n",
      "Iteration  461 :   0.0\n",
      "Iteration  462 :   0.0\n",
      "Iteration  463 :   0.0\n",
      "Iteration  464 :   0.0\n",
      "Iteration  465 :   0.0\n",
      "Iteration  466 :   0.0\n",
      "Iteration  467 :   0.0\n",
      "Iteration  468 :   0.0\n",
      "Iteration  469 :   0.0\n",
      "Iteration  470 :   0.0\n",
      "Iteration  471 :   0.0\n",
      "Iteration  472 :   0.0\n",
      "Iteration  473 :   0.0\n",
      "Iteration  474 :   0.0\n",
      "Iteration  475 :   0.0\n",
      "Iteration  476 :   0.0\n",
      "Iteration  477 :   0.0\n",
      "Iteration  478 :   0.0\n",
      "Iteration  479 :   0.0\n",
      "Iteration  480 :   0.0\n",
      "Iteration  481 :   0.0\n",
      "Iteration  482 :   0.0\n",
      "Iteration  483 :   0.0\n",
      "Iteration  484 :   0.0\n",
      "Iteration  485 :   0.0\n",
      "Iteration  486 :   0.0\n",
      "Iteration  487 :   0.0\n",
      "Iteration  488 :   0.0\n",
      "Iteration  489 :   0.0\n",
      "Iteration  490 :   0.0\n",
      "Iteration  491 :   0.0\n",
      "Iteration  492 :   0.0\n",
      "Iteration  493 :   0.0\n",
      "Iteration  494 :   0.0\n",
      "Iteration  495 :   0.0\n",
      "Iteration  496 :   0.0\n",
      "Iteration  497 :   0.0\n",
      "Iteration  498 :   0.0\n",
      "Iteration  499 :   0.0\n",
      "Iteration  500 :   0.0\n",
      "Iteration  501 :   0.0\n",
      "Iteration  502 :   0.0\n",
      "Iteration  503 :   0.0\n",
      "Iteration  504 :   0.0\n",
      "Iteration  505 :   0.0\n",
      "Iteration  506 :   0.0\n",
      "Iteration  507 :   0.0\n",
      "Iteration  508 :   0.0\n",
      "Iteration  509 :   0.0\n",
      "Iteration  510 :   0.0\n",
      "Iteration  511 :   0.0\n",
      "Iteration  512 :   0.0\n",
      "Iteration  513 :   0.0\n",
      "Iteration  514 :   0.0\n",
      "Iteration  515 :   0.0\n",
      "Iteration  516 :   0.0\n",
      "Iteration  517 :   0.0\n",
      "Iteration  518 :   0.0\n",
      "Iteration  519 :   0.0\n",
      "Iteration  520 :   0.0\n",
      "Iteration  521 :   0.0\n",
      "Iteration  522 :   0.0\n",
      "Iteration  523 :   0.0\n",
      "Iteration  524 :   0.0\n",
      "Iteration  525 :   0.0\n",
      "Iteration  526 :   0.0\n",
      "Iteration  527 :   0.0\n",
      "Iteration  528 :   0.0\n",
      "Iteration  529 :   0.0\n",
      "Iteration  530 :   0.0\n",
      "Iteration  531 :   0.0\n",
      "Iteration  532 :   0.0\n",
      "Iteration  533 :   0.0\n",
      "Iteration  534 :   0.0\n",
      "Iteration  535 :   0.0\n",
      "Iteration  536 :   0.0\n",
      "Iteration  537 :   0.0\n",
      "Iteration  538 :   0.0\n",
      "Iteration  539 :   0.0\n",
      "Iteration  540 :   0.0\n",
      "Iteration  541 :   0.0\n",
      "Iteration  542 :   0.0\n",
      "Iteration  543 :   0.0\n",
      "Iteration  544 :   0.0\n",
      "Iteration  545 :   0.0\n",
      "Iteration  546 :   0.0\n",
      "Iteration  547 :   0.0\n",
      "Iteration  548 :   0.0\n",
      "Iteration  549 :   0.0\n",
      "Iteration  550 :   0.0\n",
      "Iteration  551 :   0.0\n",
      "Iteration  552 :   0.0\n",
      "Iteration  553 :   0.0\n",
      "Iteration  554 :   0.0\n",
      "Iteration  555 :   0.0\n",
      "Iteration  556 :   0.0\n",
      "Iteration  557 :   0.0\n",
      "Iteration  558 :   0.0\n",
      "Iteration  559 :   0.0\n",
      "Iteration  560 :   0.0\n",
      "Iteration  561 :   0.0\n",
      "Iteration  562 :   0.0\n",
      "Iteration  563 :   0.0\n",
      "Iteration  564 :   0.0\n",
      "Iteration  565 :   0.0\n",
      "Iteration  566 :   0.0\n",
      "Iteration  567 :   0.0\n",
      "Iteration  568 :   0.0\n",
      "Iteration  569 :   0.0\n",
      "Iteration  570 :   0.0\n",
      "Iteration  571 :   0.0\n",
      "Iteration  572 :   0.0\n",
      "Iteration  573 :   0.0\n",
      "Iteration  574 :   0.0\n",
      "Iteration  575 :   0.0\n",
      "Iteration  576 :   0.0\n",
      "Iteration  577 :   0.0\n",
      "Iteration  578 :   0.0\n",
      "Iteration  579 :   0.0\n",
      "Iteration  580 :   0.0\n",
      "Iteration  581 :   0.0\n",
      "Iteration  582 :   0.0\n",
      "Iteration  583 :   0.0\n",
      "Iteration  584 :   0.0\n",
      "Iteration  585 :   0.0\n",
      "Iteration  586 :   0.0\n",
      "Iteration  587 :   0.0\n",
      "Iteration  588 :   0.0\n",
      "Iteration  589 :   0.0\n",
      "Iteration  590 :   0.0\n",
      "Iteration  591 :   0.0\n",
      "Iteration  592 :   0.0\n",
      "Iteration  593 :   0.0\n",
      "Iteration  594 :   0.0\n",
      "Iteration  595 :   0.0\n",
      "Iteration  596 :   0.0\n",
      "Iteration  597 :   0.0\n",
      "Iteration  598 :   0.0\n",
      "Iteration  599 :   0.0\n",
      "Iteration  600 :   0.0\n",
      "Iteration  601 :   0.0\n",
      "Iteration  602 :   0.0\n",
      "Iteration  603 :   0.0\n",
      "Iteration  604 :   0.0\n",
      "Iteration  605 :   0.0\n",
      "Iteration  606 :   0.0\n",
      "Iteration  607 :   0.0\n",
      "Iteration  608 :   0.0\n",
      "Iteration  609 :   0.0\n",
      "Iteration  610 :   0.0\n",
      "Iteration  611 :   0.0\n",
      "Iteration  612 :   0.0\n",
      "Iteration  613 :   0.0\n",
      "Iteration  614 :   0.0\n",
      "Iteration  615 :   0.0\n",
      "Iteration  616 :   0.0\n",
      "Iteration  617 :   0.0\n",
      "Iteration  618 :   0.0\n",
      "Iteration  619 :   0.0\n",
      "Iteration  620 :   0.0\n",
      "Iteration  621 :   0.0\n",
      "Iteration  622 :   0.0\n",
      "Iteration  623 :   0.0\n",
      "Iteration  624 :   0.0\n",
      "Iteration  625 :   0.0\n",
      "Iteration  626 :   0.0\n",
      "Iteration  627 :   0.0\n",
      "Iteration  628 :   0.0\n",
      "Iteration  629 :   0.0\n",
      "Iteration  630 :   0.0\n",
      "Iteration  631 :   0.0\n",
      "Iteration  632 :   0.0\n",
      "Iteration  633 :   0.0\n",
      "Iteration  634 :   0.0\n",
      "Iteration  635 :   0.0\n",
      "Iteration  636 :   0.0\n",
      "Iteration  637 :   0.0\n",
      "Iteration  638 :   0.0\n",
      "Iteration  639 :   0.0\n",
      "Iteration  640 :   0.0\n",
      "Iteration  641 :   0.0\n",
      "Iteration  642 :   0.0\n",
      "Iteration  643 :   0.0\n",
      "Iteration  644 :   0.0\n",
      "Iteration  645 :   0.0\n",
      "Iteration  646 :   0.0\n",
      "Iteration  647 :   0.0\n",
      "Iteration  648 :   0.0\n",
      "Iteration  649 :   0.0\n",
      "Iteration  650 :   0.0\n",
      "Iteration  651 :   0.0\n",
      "Iteration  652 :   0.0\n",
      "Iteration  653 :   0.0\n",
      "Iteration  654 :   0.0\n",
      "Iteration  655 :   0.0\n",
      "Iteration  656 :   0.0\n",
      "Iteration  657 :   0.0\n",
      "Iteration  658 :   0.0\n",
      "Iteration  659 :   0.0\n",
      "Iteration  660 :   0.0\n",
      "Iteration  661 :   0.0\n",
      "Iteration  662 :   0.0\n",
      "Iteration  663 :   0.0\n",
      "Iteration  664 :   0.0\n",
      "Iteration  665 :   0.0\n",
      "Iteration  666 :   0.0\n",
      "Iteration  667 :   0.0\n",
      "Iteration  668 :   0.0\n",
      "Iteration  669 :   0.0\n",
      "Iteration  670 :   0.0\n",
      "Iteration  671 :   0.0\n",
      "Iteration  672 :   0.0\n",
      "Iteration  673 :   0.0\n",
      "Iteration  674 :   0.0\n",
      "Iteration  675 :   0.0\n",
      "Iteration  676 :   0.0\n",
      "Iteration  677 :   0.0\n",
      "Iteration  678 :   0.0\n",
      "Iteration  679 :   0.0\n",
      "Iteration  680 :   0.0\n",
      "Iteration  681 :   0.0\n",
      "Iteration  682 :   0.0\n",
      "Iteration  683 :   0.0\n",
      "Iteration  684 :   0.0\n",
      "Iteration  685 :   0.0\n",
      "Iteration  686 :   0.0\n",
      "Iteration  687 :   0.0\n",
      "Iteration  688 :   0.0\n",
      "Iteration  689 :   0.0\n",
      "Iteration  690 :   0.0\n",
      "Iteration  691 :   0.0\n",
      "Iteration  692 :   0.0\n",
      "Iteration  693 :   0.0\n",
      "Iteration  694 :   0.0\n",
      "Iteration  695 :   0.0\n",
      "Iteration  696 :   0.0\n",
      "Iteration  697 :   0.0\n",
      "Iteration  698 :   0.0\n",
      "Iteration  699 :   0.0\n",
      "Iteration  700 :   0.0\n",
      "Iteration  701 :   0.0\n",
      "Iteration  702 :   0.0\n",
      "Iteration  703 :   0.0\n",
      "Iteration  704 :   0.0\n",
      "Iteration  705 :   0.0\n",
      "Iteration  706 :   0.0\n",
      "Iteration  707 :   0.0\n",
      "Iteration  708 :   0.0\n",
      "Iteration  709 :   0.0\n",
      "Iteration  710 :   0.0\n",
      "Iteration  711 :   0.0\n",
      "Iteration  712 :   0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  713 :   0.0\n",
      "Iteration  714 :   0.0\n",
      "Iteration  715 :   0.0\n",
      "Iteration  716 :   0.0\n",
      "Iteration  717 :   0.0\n",
      "Iteration  718 :   0.0\n",
      "Iteration  719 :   0.0\n",
      "Iteration  720 :   0.0\n",
      "Iteration  721 :   0.0\n",
      "Iteration  722 :   0.0\n",
      "Iteration  723 :   0.0\n",
      "Iteration  724 :   0.0\n",
      "Iteration  725 :   0.0\n",
      "Iteration  726 :   0.0\n",
      "Iteration  727 :   0.0\n",
      "Iteration  728 :   0.0\n",
      "Iteration  729 :   0.0\n",
      "Iteration  730 :   0.0\n",
      "Iteration  731 :   0.0\n",
      "Iteration  732 :   0.0\n",
      "Iteration  733 :   0.0\n",
      "Iteration  734 :   0.0\n",
      "Iteration  735 :   0.0\n",
      "Iteration  736 :   0.0\n",
      "Iteration  737 :   0.0\n",
      "Iteration  738 :   0.0\n",
      "Iteration  739 :   0.0\n",
      "Iteration  740 :   0.0\n",
      "Iteration  741 :   0.0\n",
      "******************* End of an epoch ******************************\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_epochs = 1\n",
    "minibatch_size = 32\n",
    "m = num_train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        minibatches = mini_batches(X_train_1, X_train_0, m, minibatch_size)\n",
    "\n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "\n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            _ , temp_cost, pos, neg = sess.run([optimizer, cost, score_positive, score_negative], \n",
    "                        feed_dict={X_positive:minibatch_X_positive, \n",
    "                                   X_negative:minibatch_X_negative})\n",
    "            \"\"\"\n",
    "            print(\"Epoch:\", epoch, \"Minibatch:\", i) \n",
    "            print(\"Positive score:\")\n",
    "            print(pos) \n",
    "            print(\"Negative score:\")\n",
    "            print(neg)\n",
    "            print(\"ranking loss:\", temp_cost)\n",
    "            \n",
    "            print(\"*************** End of a minibatch **********************************\")\n",
    "            \"\"\"\n",
    "            print(\"Iteration \",i, \":  \",temp_cost)\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        \n",
    "        #print(minibatch_cost)\n",
    "        print(\"******************* End of an epoch ******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
