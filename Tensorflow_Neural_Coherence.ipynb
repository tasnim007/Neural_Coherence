{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities import my_callbacks\n",
    "from utilities import data_helper\n",
    "import optparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: None\n",
      "{'S': 52415, '-': 2449404, 'O': 30440, 'X': 162058}\n",
      "Total vocabulary size in the whole dataset: 4\n",
      "['-', 'O', 'S', 'X', '0']\n"
     ]
    }
   ],
   "source": [
    "vocab = data_helper.load_all(filelist=\"final_data/wsj.all\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading entity-gird for pos and neg documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading entity-gird for pos and neg documents...\")\n",
    "\n",
    "X_train_1, X_train_0, E = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.train\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_dev_1, X_dev_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.dev\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_test_1, X_test_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.test\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "Num of traing pairs: 23744\n",
      "Num of dev pairs: 2678\n",
      "Num of test pairs: 20411\n",
      ".....................................\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_train_1)\n",
    "num_dev   = len(X_dev_1)\n",
    "num_test  = len(X_test_1)\n",
    "#assign Y value\n",
    "y_train_1 = [1] * num_train \n",
    "y_dev_1 = [1] * num_dev \n",
    "y_test_1 = [1] * num_test \n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of dev pairs: \" + str(num_dev))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "#print(\"Num of permutation in train: \" + str(opts.p_num)) \n",
    "#print(\"The maximum in length for CNN: \" + str(opts.maxlen))\n",
    "print('.....................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly shuffle the training data\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_1)\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X_positive, X_negative, vocab, E, print_ = False):\n",
    "    \"\"\"\n",
    "    Implements forward propagation of Neural coherence model\n",
    "    \n",
    "    Arguments:\n",
    "    X_positive -- A Placeholder for positive document\n",
    "    X_negative -- A Placeholder for negative document\n",
    "    vocab -- Vocabulary list of entire entity grid list\n",
    "    E -- initialized values for embedding matrix\n",
    "    print_ -- Whether size of the variables to be printed\n",
    "    \n",
    "    Returns: \n",
    "    out_positive -- Coherence Score for positive document\n",
    "    out_negative -- Coherence Score for negative document\n",
    "    parameters -- a dictionary of tensors containing trainable parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Placeholders\n",
    "    #X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    #X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    \n",
    "\n",
    "    ## First Layer of NN: Transform each grammatical role in the grid into distributed representation - a real valued vector\n",
    "    \n",
    "    \n",
    "    #Shared embedding matrix\n",
    "    #W_embedding = tf.get_variable(\"W_embedding\", [len(vocab), 100], initializer = tf.contrib.layers.xavier_initializer()) #embedding matrix \n",
    "    #E = np.float32(E) # DataType of E is float64, which is not in list of allowed values in conv1D. Allowed DataType: float16, float32\n",
    "    E =  tf.convert_to_tensor(E, tf.float32) \n",
    "    W_embedding = tf.get_variable(\"W_embedding\", initializer = E) #embedding matrix \n",
    "   \n",
    "    \n",
    "    #Look up layer\n",
    "    \n",
    "    #for positive document\n",
    "    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    embedding_negative = tf.nn.embedding_lookup(W_embedding, X_negative)\n",
    "\n",
    "\n",
    "    ## Second Layer of NN: Convolution Layer\n",
    "    \n",
    "    \n",
    "    #shared filter and bias\n",
    "    w_size = 6       #filter_size\n",
    "    emb_size = 100   #embedding_size \n",
    "    nb_filter = 150  #num_filters \n",
    "\n",
    "    filter_shape = [w_size, emb_size, nb_filter]\n",
    "\n",
    "    #W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) #filter for covolution layer 1\n",
    "    W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 2018)) #filter for covolution layer 1\n",
    "    b_conv_layer_1 =  tf.get_variable(\"b_conv_layer_1\", shape=[nb_filter], initializer = tf.constant_initializer(0.0))  #bias for convolution layer 1\n",
    "\n",
    "    \n",
    "       \n",
    "    #1D Convolution for positive document\n",
    "    conv_layer_1_positive = tf.nn.conv1d(embedding_positive, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_positive = tf.nn.bias_add(conv_layer_1_positive, b_conv_layer_1)    \n",
    "    h_conv_layer_1_positive = tf.nn.relu(conv_layer_1_with_bias_positive, name=\"relu_conv_layer_1_positive\") # Apply nonlinearity\n",
    "    \n",
    "    \n",
    "    #1D Convolution for negative document\n",
    "    conv_layer_1_negative = tf.nn.conv1d(embedding_negative, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_negative = tf.nn.bias_add(conv_layer_1_negative, b_conv_layer_1)    \n",
    "    h_conv_layer_1_negative = tf.nn.relu(conv_layer_1_with_bias_negative, name=\"relu_conv_layer_1_negative\") # Apply nonlinearity\n",
    "\n",
    "    \n",
    "\n",
    "    ## Third Layer of NN: Pooling Layer\n",
    "    \n",
    "    \n",
    "    #1D Pooling for positive document\n",
    "    m_layer_1_positive = tf.nn.pool(h_conv_layer_1_positive, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "    #1D Pooling for negative document\n",
    "    m_layer_1_negative = tf.nn.pool(h_conv_layer_1_negative, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Fourth Layer of NN: Fully Connected Layer\n",
    "    \n",
    "    #Dropout Early [As Dat Used]\n",
    "    \n",
    "    #for positive document\n",
    "    #drop_out_early_positive = tf.nn.dropout(m_layer_1_positive, keep_prob=0.5)\n",
    "    \n",
    "    #for negative document\n",
    "    #drop_out_early_negative = tf.nn.dropout(m_layer_1_negative, keep_prob=0.5)\n",
    "    \n",
    "    \n",
    "    #Flatten\n",
    "    \n",
    "    #for positive document\n",
    "    flatten_positive = tf.contrib.layers.flatten(m_layer_1_positive)\n",
    "    #flatten_positive = tf.contrib.layers.flatten(drop_out_early_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    flatten_negative = tf.contrib.layers.flatten(m_layer_1_negative)\n",
    "    #flatten_negative = tf.contrib.layers.flatten(drop_out_early_negative)\n",
    "    \n",
    "\n",
    "    #Dropout\n",
    "    \n",
    "    #for positive document\n",
    "    drop_out_positive = tf.nn.dropout(flatten_positive, keep_prob=0.5, seed=2018)\n",
    "    \n",
    "    #for negative document\n",
    "    drop_out_negative = tf.nn.dropout(flatten_negative, keep_prob=0.5, seed=2018)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Coherence Scoring\n",
    "    v_fc_layer = tf.get_variable(\"v_fc_layer\", shape = [49800, 1], initializer = tf.contrib.layers.xavier_initializer(seed = 2018)) #Weight matrix for final layer\n",
    "    b_fc_layer =  tf.get_variable(\"b_fc_layer\", shape=[1], initializer = tf.constant_initializer(0.0))  #bias for final layer\n",
    "\n",
    "    \n",
    "    \n",
    "    #for positive document\n",
    "    #out_positive = tf.contrib.layers.fully_connected(drop_out_positive, num_outputs = 1, activation_fn=None)\n",
    "    #out_positive = tf.sigmoid(out_positive)\n",
    "    out_positive = tf.add(tf.matmul(drop_out_positive, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    #for negative document\n",
    "    #out_negative = tf.contrib.layers.fully_connected(drop_out_negative, num_outputs = 1, activation_fn=None)\n",
    "    #out_negative = tf.sigmoid(out_negative)\n",
    "    out_negative = tf.add(tf.matmul(drop_out_negative, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    \n",
    "    parameters = {\"W_embedding\": W_embedding,\n",
    "                  \"W_conv_layer_1\": W_conv_layer_1,\n",
    "                  \"b_conv_layer_1\": b_conv_layer_1,\n",
    "                  \"v_fc_layer\": v_fc_layer,\n",
    "                  \"b_fc_layer\": b_fc_layer}\n",
    "    \n",
    "    \n",
    "    if(print_):\n",
    "        print(\"Layer (type)          Output Shape\")\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"\\nInputLayer:\")\n",
    "        print(\"X_positive           \",   X_positive.shape)\n",
    "        print(\"X_negative           \",   X_negative.shape)\n",
    "        print(\"\\nEmbedding Layer:\")\n",
    "        print(\"Embedding Matrix     \",   W_embedding.shape)\n",
    "        print(\"Embedding Positive   \",   embedding_positive.shape)\n",
    "        print(\"Embedding Negative   \",   embedding_negative.shape)\n",
    "        print(\"\\nConvolution 1D Layer:\")\n",
    "        print(\"Filter Shape         \",   W_conv_layer_1.shape)\n",
    "        print(\"Conv Positive        \",   h_conv_layer_1_positive.shape)\n",
    "        print(\"Conv Negative        \",   h_conv_layer_1_negative.shape)\n",
    "        print(\"\\nMax Pooling 1D Layer:\")\n",
    "        print(\"MaxPool Positive     \",   m_layer_1_positive.shape)\n",
    "        print(\"MaxPool Negative     \",   m_layer_1_negative.shape)\n",
    "        print(\"\\nFlatten Layer: \")\n",
    "        print(\"Flatten Positive     \",   flatten_positive.shape)\n",
    "        print(\"Flatten Negative     \",   flatten_negative.shape)\n",
    "        print(\"\\nDropout Layer: \")\n",
    "        print(\"Dropout Positive     \",   drop_out_positive.shape)\n",
    "        print(\"Dropout Negative     \",   drop_out_negative.shape)\n",
    "        print(\"\\nFully Connected Layer:\")\n",
    "        print(\"FC Positive          \",   out_positive.shape)\n",
    "        print(\"FC Negative          \",   out_negative.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return out_positive, out_negative, parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(pos, neg):\n",
    "    \"\"\"\n",
    "    Implements the ranking objective.\n",
    "    \n",
    "    Arguments:\n",
    "    pos -- score for positive document batch\n",
    "    neg -- score for negative document batch\n",
    "    \n",
    "    Returns:\n",
    "    Average ranking loss for the batch  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.maximum(1.0 + neg - pos, 0.0) \n",
    "    #print(loss)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, mini_batch_size = 32):\n",
    "    \"\"\"\n",
    "    Creates minibatches.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Positive Documents\n",
    "    Y -- Negative Documents\n",
    "    mini_batch_size -- Size of each mini batch. \n",
    "    \n",
    "    Returns:\n",
    "    list of mini batches from the positive and negative documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    m = len(X)   \n",
    "    mini_batches = []\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)          Output Shape\n",
      "_________________________________________\n",
      "\n",
      "InputLayer:\n",
      "X_positive            (?, 2000)\n",
      "X_negative            (?, 2000)\n",
      "\n",
      "Embedding Layer:\n",
      "Embedding Matrix      (5, 100)\n",
      "Embedding Positive    (?, 2000, 100)\n",
      "Embedding Negative    (?, 2000, 100)\n",
      "\n",
      "Convolution 1D Layer:\n",
      "Filter Shape          (6, 100, 150)\n",
      "Conv Positive         (?, 1995, 150)\n",
      "Conv Negative         (?, 1995, 150)\n",
      "\n",
      "Max Pooling 1D Layer:\n",
      "MaxPool Positive      (?, 332, 150)\n",
      "MaxPool Negative      (?, 332, 150)\n",
      "\n",
      "Flatten Layer: \n",
      "Flatten Positive      (?, 49800)\n",
      "Flatten Negative      (?, 49800)\n",
      "\n",
      "Dropout Layer: \n",
      "Dropout Positive      (?, 49800)\n",
      "Dropout Negative      (?, 49800)\n",
      "\n",
      "Fully Connected Layer:\n",
      "FC Positive           (?, 1)\n",
      "FC Negative           (?, 1)\n"
     ]
    }
   ],
   "source": [
    "## Create Placeholders\n",
    "X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for positive document\n",
    "X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for negative document\n",
    "\n",
    "# Forward propagation\n",
    "score_positive, score_negative, parameters = forward_propagation(X_positive, X_negative, vocab, E, print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function:\n",
    "cost = ranking_loss(score_positive, score_negative)\n",
    "\n",
    "# Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.0, momentum=0.9, epsilon=1e-8).minimize(cost)\n",
    "\n",
    "\n",
    "## Using keras RMSProp\n",
    "\n",
    "W_embedding = parameters[\"W_embedding\"]\n",
    "W_conv_layer_1 = parameters[\"W_conv_layer_1\"]\n",
    "b_conv_layer_1 = parameters[\"b_conv_layer_1\"]\n",
    "v_fc_layer = parameters[\"v_fc_layer\"]\n",
    "b_fc_layer = parameters[\"b_fc_layer\"]\n",
    "optimizer = tf.keras.optimizers.RMSprop().get_updates(cost, [W_embedding, W_conv_layer_1, b_conv_layer_1, v_fc_layer, b_fc_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_1 = X_train_1[:100, :]\\nX_train_0 = X_train_0[:100, :]\\nX_test_1 = X_test_1[:100, :]\\nX_test_0 = X_test_0[:100, :]\\n\\n\\nnum_train = len(X_train_1)\\nnum_test  = len(X_test_1)\\n\\n\\nprint(\\'.....................................\\')\\nprint(\"Num of traing pairs: \" + str(num_train))\\nprint(\"Num of test pairs: \" + str(num_test))\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train_1 = X_train_1[:100, :]\n",
    "X_train_0 = X_train_0[:100, :]\n",
    "X_test_1 = X_test_1[:100, :]\n",
    "X_test_0 = X_test_0[:100, :]\n",
    "\n",
    "\n",
    "num_train = len(X_train_1)\n",
    "num_test  = len(X_test_1)\n",
    "\n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 :   1.00031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tasnim/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:42: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 :   1.00034\n",
      "Iteration  2 :   0.999933\n",
      "Iteration  3 :   0.996796\n",
      "Iteration  4 :   0.99645\n",
      "Iteration  5 :   0.995236\n",
      "Iteration  6 :   0.989544\n",
      "Iteration  7 :   0.996985\n",
      "Iteration  8 :   0.997288\n",
      "Iteration  9 :   0.980418\n",
      "Iteration  10 :   0.969542\n",
      "Iteration  11 :   0.973834\n",
      "Iteration  12 :   0.978689\n",
      "Iteration  13 :   0.956851\n",
      "Iteration  14 :   0.915783\n",
      "Iteration  15 :   0.915817\n",
      "Iteration  16 :   0.925021\n",
      "Iteration  17 :   0.925157\n",
      "Iteration  18 :   0.905358\n",
      "Iteration  19 :   0.896002\n",
      "Iteration  20 :   0.848196\n",
      "Iteration  21 :   0.846935\n",
      "Iteration  22 :   0.827453\n",
      "Iteration  23 :   0.801131\n",
      "Iteration  24 :   0.719103\n",
      "Iteration  25 :   0.707709\n",
      "Iteration  26 :   0.787961\n",
      "Iteration  27 :   0.740446\n",
      "Iteration  28 :   0.731502\n",
      "Iteration  29 :   0.660232\n",
      "Iteration  30 :   0.597832\n",
      "Iteration  31 :   0.493742\n",
      "Iteration  32 :   0.495878\n",
      "Iteration  33 :   0.557759\n",
      "Iteration  34 :   0.548353\n",
      "Iteration  35 :   0.561471\n",
      "Iteration  36 :   0.678467\n",
      "Iteration  37 :   0.427315\n",
      "Iteration  38 :   0.517657\n",
      "Iteration  39 :   0.622403\n",
      "Iteration  40 :   0.291453\n",
      "Iteration  41 :   0.333321\n",
      "Iteration  42 :   0.359761\n",
      "Iteration  43 :   0.461455\n",
      "Iteration  44 :   0.441647\n",
      "Iteration  45 :   0.485504\n",
      "Iteration  46 :   0.331444\n",
      "Iteration  47 :   0.373026\n",
      "Iteration  48 :   0.313274\n",
      "Iteration  49 :   0.26626\n",
      "Iteration  50 :   0.466875\n",
      "Iteration  51 :   0.456638\n",
      "Iteration  52 :   0.390103\n",
      "Iteration  53 :   0.426884\n",
      "Iteration  54 :   0.606615\n",
      "Iteration  55 :   0.397702\n",
      "Iteration  56 :   0.426716\n",
      "Iteration  57 :   0.324809\n",
      "Iteration  58 :   0.500117\n",
      "Iteration  59 :   0.228537\n",
      "Iteration  60 :   0.35627\n",
      "Iteration  61 :   0.490254\n",
      "Iteration  62 :   0.526784\n",
      "Iteration  63 :   0.324184\n",
      "Iteration  64 :   0.424423\n",
      "Iteration  65 :   0.335115\n",
      "Iteration  66 :   0.292574\n",
      "Iteration  67 :   0.312201\n",
      "Iteration  68 :   0.315391\n",
      "Iteration  69 :   0.265396\n",
      "Iteration  70 :   0.459674\n",
      "Iteration  71 :   0.305674\n",
      "Iteration  72 :   0.34319\n",
      "Iteration  73 :   0.414379\n",
      "Iteration  74 :   0.376485\n",
      "Iteration  75 :   0.407822\n",
      "Iteration  76 :   0.60402\n",
      "Iteration  77 :   0.240216\n",
      "Iteration  78 :   0.297859\n",
      "Iteration  79 :   0.382852\n",
      "Iteration  80 :   0.39795\n",
      "Iteration  81 :   0.384615\n",
      "Iteration  82 :   0.178613\n",
      "Iteration  83 :   0.342345\n",
      "Iteration  84 :   0.349207\n",
      "Iteration  85 :   0.271909\n",
      "Iteration  86 :   0.114041\n",
      "Iteration  87 :   0.474847\n",
      "Iteration  88 :   0.675154\n",
      "Iteration  89 :   0.3571\n",
      "Iteration  90 :   0.461535\n",
      "Iteration  91 :   0.284074\n",
      "Iteration  92 :   0.125863\n",
      "Iteration  93 :   0.329764\n",
      "Iteration  94 :   0.306255\n",
      "Iteration  95 :   0.269732\n",
      "Iteration  96 :   0.178699\n",
      "Iteration  97 :   0.252636\n",
      "Iteration  98 :   0.278379\n",
      "Iteration  99 :   0.401078\n",
      "Iteration  100 :   0.32673\n",
      "Iteration  101 :   0.11189\n",
      "Iteration  102 :   0.167686\n",
      "Iteration  103 :   0.269377\n",
      "Iteration  104 :   0.597793\n",
      "Iteration  105 :   0.361762\n",
      "Iteration  106 :   0.215122\n",
      "Iteration  107 :   0.32859\n",
      "Iteration  108 :   0.283038\n",
      "Iteration  109 :   0.485985\n",
      "Iteration  110 :   0.570724\n",
      "Iteration  111 :   0.220825\n",
      "Iteration  112 :   0.364175\n",
      "Iteration  113 :   0.219175\n",
      "Iteration  114 :   0.397115\n",
      "Iteration  115 :   0.353942\n",
      "Iteration  116 :   0.333666\n",
      "Iteration  117 :   0.34708\n",
      "Iteration  118 :   0.219987\n",
      "Iteration  119 :   0.249893\n",
      "Iteration  120 :   0.281261\n",
      "Iteration  121 :   0.293506\n",
      "Iteration  122 :   0.211704\n",
      "Iteration  123 :   0.303478\n",
      "Iteration  124 :   0.478818\n",
      "Iteration  125 :   0.161846\n",
      "Iteration  126 :   0.307112\n",
      "Iteration  127 :   0.0936196\n",
      "Iteration  128 :   0.16512\n",
      "Iteration  129 :   0.208065\n",
      "Iteration  130 :   0.45789\n",
      "Iteration  131 :   0.334019\n",
      "Iteration  132 :   0.187683\n",
      "Iteration  133 :   0.12789\n",
      "Iteration  134 :   0.499643\n",
      "Iteration  135 :   0.325482\n",
      "Iteration  136 :   0.133137\n",
      "Iteration  137 :   0.427792\n",
      "Iteration  138 :   0.122017\n",
      "Iteration  139 :   0.282062\n",
      "Iteration  140 :   0.141507\n",
      "Iteration  141 :   0.346366\n",
      "Iteration  142 :   0.308268\n",
      "Iteration  143 :   0.354077\n",
      "Iteration  144 :   0.435242\n",
      "Iteration  145 :   0.558164\n",
      "Iteration  146 :   0.443334\n",
      "Iteration  147 :   0.18904\n",
      "Iteration  148 :   0.457932\n",
      "Iteration  149 :   0.127435\n",
      "Iteration  150 :   0.160984\n",
      "Iteration  151 :   0.200318\n",
      "Iteration  152 :   0.462262\n",
      "Iteration  153 :   0.378021\n",
      "Iteration  154 :   0.427876\n",
      "Iteration  155 :   0.239405\n",
      "Iteration  156 :   0.0664284\n",
      "Iteration  157 :   0.302801\n",
      "Iteration  158 :   0.188966\n",
      "Iteration  159 :   0.314451\n",
      "Iteration  160 :   0.261879\n",
      "Iteration  161 :   0.40745\n",
      "Iteration  162 :   0.383064\n",
      "Iteration  163 :   0.333773\n",
      "Iteration  164 :   0.298287\n",
      "Iteration  165 :   0.169673\n",
      "Iteration  166 :   0.309701\n",
      "Iteration  167 :   0.385293\n",
      "Iteration  168 :   0.347132\n",
      "Iteration  169 :   0.279855\n",
      "Iteration  170 :   0.242948\n",
      "Iteration  171 :   0.175602\n",
      "Iteration  172 :   0.250212\n",
      "Iteration  173 :   0.26866\n",
      "Iteration  174 :   0.362539\n",
      "Iteration  175 :   0.206471\n",
      "Iteration  176 :   0.146501\n",
      "Iteration  177 :   0.287048\n",
      "Iteration  178 :   0.190287\n",
      "Iteration  179 :   0.239241\n",
      "Iteration  180 :   0.063674\n",
      "Iteration  181 :   0.485764\n",
      "Iteration  182 :   0.129457\n",
      "Iteration  183 :   0.254206\n",
      "Iteration  184 :   0.269455\n",
      "Iteration  185 :   0.358069\n",
      "Iteration  186 :   0.323423\n",
      "Iteration  187 :   0.185254\n",
      "Iteration  188 :   0.0553032\n",
      "Iteration  189 :   0.206107\n",
      "Iteration  190 :   0.445399\n",
      "Iteration  191 :   0.271045\n",
      "Iteration  192 :   0.185825\n",
      "Iteration  193 :   0.280979\n",
      "Iteration  194 :   0.311431\n",
      "Iteration  195 :   0.101386\n",
      "Iteration  196 :   0.336322\n",
      "Iteration  197 :   0.428849\n",
      "Iteration  198 :   0.246912\n",
      "Iteration  199 :   0.126752\n",
      "Iteration  200 :   0.0454522\n",
      "Iteration  201 :   0.24536\n",
      "Iteration  202 :   0.0984725\n",
      "Iteration  203 :   0.211435\n",
      "Iteration  204 :   0.393811\n",
      "Iteration  205 :   0.0937783\n",
      "Iteration  206 :   0.239547\n",
      "Iteration  207 :   0.22959\n",
      "Iteration  208 :   0.180429\n",
      "Iteration  209 :   0.256472\n",
      "Iteration  210 :   0.148929\n",
      "Iteration  211 :   0.312806\n",
      "Iteration  212 :   0.195752\n",
      "Iteration  213 :   0.150826\n",
      "Iteration  214 :   0.260391\n",
      "Iteration  215 :   0.234799\n",
      "Iteration  216 :   0.17936\n",
      "Iteration  217 :   0.197243\n",
      "Iteration  218 :   0.184299\n",
      "Iteration  219 :   0.276099\n",
      "Iteration  220 :   0.588846\n",
      "Iteration  221 :   0.272264\n",
      "Iteration  222 :   0.239873\n",
      "Iteration  223 :   0.212857\n",
      "Iteration  224 :   0.192001\n",
      "Iteration  225 :   0.304134\n",
      "Iteration  226 :   0.215083\n",
      "Iteration  227 :   0.34732\n",
      "Iteration  228 :   0.183246\n",
      "Iteration  229 :   0.118139\n",
      "Iteration  230 :   0.347733\n",
      "Iteration  231 :   0.240569\n",
      "Iteration  232 :   0.0698087\n",
      "Iteration  233 :   0.0728757\n",
      "Iteration  234 :   0.187343\n",
      "Iteration  235 :   0.224371\n",
      "Iteration  236 :   0.307388\n",
      "Iteration  237 :   0.153461\n",
      "Iteration  238 :   0.429112\n",
      "Iteration  239 :   0.0345261\n",
      "Iteration  240 :   0.23788\n",
      "Iteration  241 :   0.15962\n",
      "Iteration  242 :   0.198207\n",
      "Iteration  243 :   0.298545\n",
      "Iteration  244 :   0.228334\n",
      "Iteration  245 :   0.27972\n",
      "Iteration  246 :   0.417221\n",
      "Iteration  247 :   0.266098\n",
      "Iteration  248 :   0.278904\n",
      "Iteration  249 :   0.260115\n",
      "Iteration  250 :   0.286164\n",
      "Iteration  251 :   0.158148\n",
      "Iteration  252 :   0.358078\n",
      "Iteration  253 :   0.275171\n",
      "Iteration  254 :   0.114931\n",
      "Iteration  255 :   0.18159\n",
      "Iteration  256 :   0.156103\n",
      "Iteration  257 :   0.384833\n",
      "Iteration  258 :   0.221819\n",
      "Iteration  259 :   0.121586\n",
      "Iteration  260 :   0.225414\n",
      "Iteration  261 :   0.344256\n",
      "Iteration  262 :   0.220231\n",
      "Iteration  263 :   0.243304\n",
      "Iteration  264 :   0.149812\n",
      "Iteration  265 :   0.26849\n",
      "Iteration  266 :   0.227296\n",
      "Iteration  267 :   0.145639\n",
      "Iteration  268 :   0.219161\n",
      "Iteration  269 :   0.202234\n",
      "Iteration  270 :   0.31486\n",
      "Iteration  271 :   0.223079\n",
      "Iteration  272 :   0.259861\n",
      "Iteration  273 :   0.334533\n",
      "Iteration  274 :   0.165378\n",
      "Iteration  275 :   0.36429\n",
      "Iteration  276 :   0.273837\n",
      "Iteration  277 :   0.23808\n",
      "Iteration  278 :   0.182783\n",
      "Iteration  279 :   0.165401\n",
      "Iteration  280 :   0.254409\n",
      "Iteration  281 :   0.227465\n",
      "Iteration  282 :   0.34888\n",
      "Iteration  283 :   0.215442\n",
      "Iteration  284 :   0.12877\n",
      "Iteration  285 :   0.321878\n",
      "Iteration  286 :   0.274362\n",
      "Iteration  287 :   0.162392\n",
      "Iteration  288 :   0.194072\n",
      "Iteration  289 :   0.136776\n",
      "Iteration  290 :   0.249381\n",
      "Iteration  291 :   0.0645938\n",
      "Iteration  292 :   0.190579\n",
      "Iteration  293 :   0.108288\n",
      "Iteration  294 :   0.225394\n",
      "Iteration  295 :   0.422369\n",
      "Iteration  296 :   0.135043\n",
      "Iteration  297 :   0.354642\n",
      "Iteration  298 :   0.136095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  299 :   0.443947\n",
      "Iteration  300 :   0.16748\n",
      "Iteration  301 :   0.395949\n",
      "Iteration  302 :   0.130437\n",
      "Iteration  303 :   0.37054\n",
      "Iteration  304 :   0.17084\n",
      "Iteration  305 :   0.52354\n",
      "Iteration  306 :   0.322959\n",
      "Iteration  307 :   0.117723\n",
      "Iteration  308 :   0.261638\n",
      "Iteration  309 :   0.235293\n",
      "Iteration  310 :   0.412146\n",
      "Iteration  311 :   0.160367\n",
      "Iteration  312 :   0.196979\n",
      "Iteration  313 :   0.0355089\n",
      "Iteration  314 :   0.32905\n",
      "Iteration  315 :   0.271852\n",
      "Iteration  316 :   0.142261\n",
      "Iteration  317 :   0.327174\n",
      "Iteration  318 :   0.134738\n",
      "Iteration  319 :   0.16805\n",
      "Iteration  320 :   0.239435\n",
      "Iteration  321 :   0.248101\n",
      "Iteration  322 :   0.0883594\n",
      "Iteration  323 :   0.189641\n",
      "Iteration  324 :   0.353189\n",
      "Iteration  325 :   0.322678\n",
      "Iteration  326 :   0.0704272\n",
      "Iteration  327 :   0.331277\n",
      "Iteration  328 :   0.103396\n",
      "Iteration  329 :   0.273302\n",
      "Iteration  330 :   0.231561\n",
      "Iteration  331 :   0.342465\n",
      "Iteration  332 :   0.200452\n",
      "Iteration  333 :   0.105461\n",
      "Iteration  334 :   0.0936356\n",
      "Iteration  335 :   0.181843\n",
      "Iteration  336 :   0.360699\n",
      "Iteration  337 :   0.309934\n",
      "Iteration  338 :   0.338612\n",
      "Iteration  339 :   0.178369\n",
      "Iteration  340 :   0.446337\n",
      "Iteration  341 :   0.240621\n",
      "Iteration  342 :   0.271618\n",
      "Iteration  343 :   0.0549603\n",
      "Iteration  344 :   0.258412\n",
      "Iteration  345 :   0.202094\n",
      "Iteration  346 :   0.147526\n",
      "Iteration  347 :   0.279443\n",
      "Iteration  348 :   0.183101\n",
      "Iteration  349 :   0.233223\n",
      "Iteration  350 :   0.347248\n",
      "Iteration  351 :   0.203636\n",
      "Iteration  352 :   0.358551\n",
      "Iteration  353 :   0.240129\n",
      "Iteration  354 :   0.134402\n",
      "Iteration  355 :   0.153092\n",
      "Iteration  356 :   0.424568\n",
      "Iteration  357 :   0.388411\n",
      "Iteration  358 :   0.170139\n",
      "Iteration  359 :   0.00270391\n",
      "Iteration  360 :   0.0935349\n",
      "Iteration  361 :   0.319245\n",
      "Iteration  362 :   0.28777\n",
      "Iteration  363 :   0.00364947\n",
      "Iteration  364 :   0.144107\n",
      "Iteration  365 :   0.265048\n",
      "Iteration  366 :   0.298751\n",
      "Iteration  367 :   0.305614\n",
      "Iteration  368 :   0.174402\n",
      "Iteration  369 :   0.284635\n",
      "Iteration  370 :   0.181623\n",
      "Iteration  371 :   0.275325\n",
      "Iteration  372 :   0.161911\n",
      "Iteration  373 :   0.255059\n",
      "Iteration  374 :   0.253339\n",
      "Iteration  375 :   0.040076\n",
      "Iteration  376 :   0.243779\n",
      "Iteration  377 :   0.321299\n",
      "Iteration  378 :   0.00569296\n",
      "Iteration  379 :   0.251336\n",
      "Iteration  380 :   0.164786\n",
      "Iteration  381 :   0.224783\n",
      "Iteration  382 :   0.447928\n",
      "Iteration  383 :   0.330815\n",
      "Iteration  384 :   0.262757\n",
      "Iteration  385 :   0.235341\n",
      "Iteration  386 :   0.143649\n",
      "Iteration  387 :   0.055619\n",
      "Iteration  388 :   0.206956\n",
      "Iteration  389 :   0.0486286\n",
      "Iteration  390 :   0.210713\n",
      "Iteration  391 :   0.125714\n",
      "Iteration  392 :   0.393468\n",
      "Iteration  393 :   0.400736\n",
      "Iteration  394 :   0.105522\n",
      "Iteration  395 :   0.375626\n",
      "Iteration  396 :   0.210069\n",
      "Iteration  397 :   0.281031\n",
      "Iteration  398 :   0.261567\n",
      "Iteration  399 :   0.226053\n",
      "Iteration  400 :   0.217855\n",
      "Iteration  401 :   0.375048\n",
      "Iteration  402 :   0.226861\n",
      "Iteration  403 :   0.127834\n",
      "Iteration  404 :   0.0155058\n",
      "Iteration  405 :   0.13455\n",
      "Iteration  406 :   0.180472\n",
      "Iteration  407 :   0.142583\n",
      "Iteration  408 :   0.23908\n",
      "Iteration  409 :   0.340403\n",
      "Iteration  410 :   0.221334\n",
      "Iteration  411 :   0.0061233\n",
      "Iteration  412 :   0.433519\n",
      "Iteration  413 :   0.311684\n",
      "Iteration  414 :   0.133006\n",
      "Iteration  415 :   0.0664356\n",
      "Iteration  416 :   0.187944\n",
      "Iteration  417 :   0.201024\n",
      "Iteration  418 :   0.169142\n",
      "Iteration  419 :   0.335092\n",
      "Iteration  420 :   0.146971\n",
      "Iteration  421 :   0.15226\n",
      "Iteration  422 :   0.117046\n",
      "Iteration  423 :   0.111927\n",
      "Iteration  424 :   0.25254\n",
      "Iteration  425 :   0.147887\n",
      "Iteration  426 :   0.151262\n",
      "Iteration  427 :   0.277698\n",
      "Iteration  428 :   0.143349\n",
      "Iteration  429 :   0.392482\n",
      "Iteration  430 :   0.240889\n",
      "Iteration  431 :   0.178311\n",
      "Iteration  432 :   0.203228\n",
      "Iteration  433 :   0.283736\n",
      "Iteration  434 :   0.104161\n",
      "Iteration  435 :   0.206078\n",
      "Iteration  436 :   0.156811\n",
      "Iteration  437 :   0.185099\n",
      "Iteration  438 :   0.254804\n",
      "Iteration  439 :   0.216691\n",
      "Iteration  440 :   0.0656099\n",
      "Iteration  441 :   0.275819\n",
      "Iteration  442 :   0.337839\n",
      "Iteration  443 :   0.235407\n",
      "Iteration  444 :   0.329181\n",
      "Iteration  445 :   0.0976191\n",
      "Iteration  446 :   0.0983658\n",
      "Iteration  447 :   0.284543\n",
      "Iteration  448 :   0.208292\n",
      "Iteration  449 :   0.14413\n",
      "Iteration  450 :   0.119057\n",
      "Iteration  451 :   0.187897\n",
      "Iteration  452 :   0.0773695\n",
      "Iteration  453 :   0.0266857\n",
      "Iteration  454 :   0.159898\n",
      "Iteration  455 :   0.236087\n",
      "Iteration  456 :   0.365226\n",
      "Iteration  457 :   0.18902\n",
      "Iteration  458 :   0.143225\n",
      "Iteration  459 :   0.110205\n",
      "Iteration  460 :   0.323016\n",
      "Iteration  461 :   0.136746\n",
      "Iteration  462 :   0.515018\n",
      "Iteration  463 :   0.232584\n",
      "Iteration  464 :   0.071537\n",
      "Iteration  465 :   0.131486\n",
      "Iteration  466 :   0.190787\n",
      "Iteration  467 :   0.183276\n",
      "Iteration  468 :   0.251621\n",
      "Iteration  469 :   0.197119\n",
      "Iteration  470 :   0.323628\n",
      "Iteration  471 :   0.17172\n",
      "Iteration  472 :   0.152155\n",
      "Iteration  473 :   0.411908\n",
      "Iteration  474 :   0.244241\n",
      "Iteration  475 :   0.358262\n",
      "Iteration  476 :   0.212288\n",
      "Iteration  477 :   0.116237\n",
      "Iteration  478 :   0.203279\n",
      "Iteration  479 :   0.110017\n",
      "Iteration  480 :   0.287344\n",
      "Iteration  481 :   0.137609\n",
      "Iteration  482 :   0.26858\n",
      "Iteration  483 :   0.142593\n",
      "Iteration  484 :   0.132436\n",
      "Iteration  485 :   0.0691776\n",
      "Iteration  486 :   0.165285\n",
      "Iteration  487 :   0.221145\n",
      "Iteration  488 :   0.133652\n",
      "Iteration  489 :   0.21418\n",
      "Iteration  490 :   0.0961747\n",
      "Iteration  491 :   0.262834\n",
      "Iteration  492 :   0.234084\n",
      "Iteration  493 :   0.184331\n",
      "Iteration  494 :   0.24605\n",
      "Iteration  495 :   0.205137\n",
      "Iteration  496 :   0.301071\n",
      "Iteration  497 :   0.167753\n",
      "Iteration  498 :   0.206646\n",
      "Iteration  499 :   0.119657\n",
      "Iteration  500 :   0.0308242\n",
      "Iteration  501 :   0.0485268\n",
      "Iteration  502 :   0.262292\n",
      "Iteration  503 :   0.089349\n",
      "Iteration  504 :   0.257722\n",
      "Iteration  505 :   0.415541\n",
      "Iteration  506 :   0.202563\n",
      "Iteration  507 :   0.257865\n",
      "Iteration  508 :   0.205481\n",
      "Iteration  509 :   0.175882\n",
      "Iteration  510 :   0.0987899\n",
      "Iteration  511 :   0.184866\n",
      "Iteration  512 :   0.275935\n",
      "Iteration  513 :   0.190308\n",
      "Iteration  514 :   0.156934\n",
      "Iteration  515 :   0.214221\n",
      "Iteration  516 :   0.103014\n",
      "Iteration  517 :   0.25506\n",
      "Iteration  518 :   0.0581927\n",
      "Iteration  519 :   0.219648\n",
      "Iteration  520 :   0.462015\n",
      "Iteration  521 :   0.178724\n",
      "Iteration  522 :   0.172026\n",
      "Iteration  523 :   0.161507\n",
      "Iteration  524 :   0.298998\n",
      "Iteration  525 :   0.322152\n",
      "Iteration  526 :   0.213264\n",
      "Iteration  527 :   0.140492\n",
      "Iteration  528 :   0.287128\n",
      "Iteration  529 :   0.234226\n",
      "Iteration  530 :   0.164756\n",
      "Iteration  531 :   0.0967195\n",
      "Iteration  532 :   0.291285\n",
      "Iteration  533 :   0.109404\n",
      "Iteration  534 :   0.148763\n",
      "Iteration  535 :   0.0262234\n",
      "Iteration  536 :   0.141585\n",
      "Iteration  537 :   0.132268\n",
      "Iteration  538 :   0.0513513\n",
      "Iteration  539 :   0.210637\n",
      "Iteration  540 :   0.177511\n",
      "Iteration  541 :   0.223684\n",
      "Iteration  542 :   0.147187\n",
      "Iteration  543 :   0.237323\n",
      "Iteration  544 :   0.0748904\n",
      "Iteration  545 :   0.107943\n",
      "Iteration  546 :   0.118456\n",
      "Iteration  547 :   0.403224\n",
      "Iteration  548 :   0.37172\n",
      "Iteration  549 :   0.0598998\n",
      "Iteration  550 :   0.095207\n",
      "Iteration  551 :   0.276169\n",
      "Iteration  552 :   0.268276\n",
      "Iteration  553 :   0.207432\n",
      "Iteration  554 :   0.152717\n",
      "Iteration  555 :   0.128291\n",
      "Iteration  556 :   0.295109\n",
      "Iteration  557 :   0.175442\n",
      "Iteration  558 :   0.0817735\n",
      "Iteration  559 :   0.185694\n",
      "Iteration  560 :   0.050621\n",
      "Iteration  561 :   0.0925741\n",
      "Iteration  562 :   0.201399\n",
      "Iteration  563 :   0.205774\n",
      "Iteration  564 :   0.073488\n",
      "Iteration  565 :   0.140483\n",
      "Iteration  566 :   0.0728016\n",
      "Iteration  567 :   0.282472\n",
      "Iteration  568 :   0.418214\n",
      "Iteration  569 :   0.126241\n",
      "Iteration  570 :   0.0768785\n",
      "Iteration  571 :   0.393432\n",
      "Iteration  572 :   0.458815\n",
      "Iteration  573 :   0.136688\n",
      "Iteration  574 :   0.175447\n",
      "Iteration  575 :   0.138838\n",
      "Iteration  576 :   0.216592\n",
      "Iteration  577 :   0.0573323\n",
      "Iteration  578 :   0.161948\n",
      "Iteration  579 :   0.265484\n",
      "Iteration  580 :   0.224753\n",
      "Iteration  581 :   0.104465\n",
      "Iteration  582 :   0.025898\n",
      "Iteration  583 :   0.192425\n",
      "Iteration  584 :   0.150839\n",
      "Iteration  585 :   0.237518\n",
      "Iteration  586 :   0.0351949\n",
      "Iteration  587 :   0.32391\n",
      "Iteration  588 :   0.0215166\n",
      "Iteration  589 :   0.234013\n",
      "Iteration  590 :   0.419455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  591 :   0.249045\n",
      "Iteration  592 :   0.126978\n",
      "Iteration  593 :   0.245895\n",
      "Iteration  594 :   0.270346\n",
      "Iteration  595 :   0.149601\n",
      "Iteration  596 :   0.184495\n",
      "Iteration  597 :   0.29692\n",
      "Iteration  598 :   0.0524721\n",
      "Iteration  599 :   0.0498137\n",
      "Iteration  600 :   0.00268316\n",
      "Iteration  601 :   0.359878\n",
      "Iteration  602 :   0.0284119\n",
      "Iteration  603 :   0.512002\n",
      "Iteration  604 :   0.00976515\n",
      "Iteration  605 :   0.0944705\n",
      "Iteration  606 :   0.0818596\n",
      "Iteration  607 :   0.142551\n",
      "Iteration  608 :   0.23253\n",
      "Iteration  609 :   0.314334\n",
      "Iteration  610 :   0.329693\n",
      "Iteration  611 :   0.218377\n",
      "Iteration  612 :   0.189505\n",
      "Iteration  613 :   0.283545\n",
      "Iteration  614 :   0.231134\n",
      "Iteration  615 :   0.137554\n",
      "Iteration  616 :   0.273521\n",
      "Iteration  617 :   0.00856686\n",
      "Iteration  618 :   0.0104585\n",
      "Iteration  619 :   0.133027\n",
      "Iteration  620 :   0.105552\n",
      "Iteration  621 :   0.128106\n",
      "Iteration  622 :   0.186766\n",
      "Iteration  623 :   0.146196\n",
      "Iteration  624 :   0.417335\n",
      "Iteration  625 :   0.152648\n",
      "Iteration  626 :   0.176966\n",
      "Iteration  627 :   0.108126\n",
      "Iteration  628 :   0.0298314\n",
      "Iteration  629 :   0.144619\n",
      "Iteration  630 :   0.304775\n",
      "Iteration  631 :   0.201406\n",
      "Iteration  632 :   0.222554\n",
      "Iteration  633 :   0.265744\n",
      "Iteration  634 :   0.108119\n",
      "Iteration  635 :   0.156621\n",
      "Iteration  636 :   0.0490437\n",
      "Iteration  637 :   0.171168\n",
      "Iteration  638 :   0.294909\n",
      "Iteration  639 :   0.214759\n",
      "Iteration  640 :   0.157043\n",
      "Iteration  641 :   0.156607\n",
      "Iteration  642 :   0.204884\n",
      "Iteration  643 :   0.198521\n",
      "Iteration  644 :   0.153029\n",
      "Iteration  645 :   0.293045\n",
      "Iteration  646 :   0.126969\n",
      "Iteration  647 :   0.136603\n",
      "Iteration  648 :   0.2573\n",
      "Iteration  649 :   0.25433\n",
      "Iteration  650 :   0.206221\n",
      "Iteration  651 :   0.0353584\n",
      "Iteration  652 :   0.469929\n",
      "Iteration  653 :   0.257492\n",
      "Iteration  654 :   0.24987\n",
      "Iteration  655 :   0.0178566\n",
      "Iteration  656 :   0.109441\n",
      "Iteration  657 :   0.0863376\n",
      "Iteration  658 :   0.147426\n",
      "Iteration  659 :   0.138894\n",
      "Iteration  660 :   0.201309\n",
      "Iteration  661 :   0.193711\n",
      "Iteration  662 :   0.157784\n",
      "Iteration  663 :   0.109478\n",
      "Iteration  664 :   0.0873456\n",
      "Iteration  665 :   0.0722837\n",
      "Iteration  666 :   0.20097\n",
      "Iteration  667 :   0.326502\n",
      "Iteration  668 :   0.144044\n",
      "Iteration  669 :   0.286573\n",
      "Iteration  670 :   0.134881\n",
      "Iteration  671 :   0.284119\n",
      "Iteration  672 :   0.143645\n",
      "Iteration  673 :   0.27309\n",
      "Iteration  674 :   0.18807\n",
      "Iteration  675 :   0.0755506\n",
      "Iteration  676 :   0.104127\n",
      "Iteration  677 :   0.0848503\n",
      "Iteration  678 :   0.0806363\n",
      "Iteration  679 :   0.200788\n",
      "Iteration  680 :   0.0750289\n",
      "Iteration  681 :   0.416138\n",
      "Iteration  682 :   0.288024\n",
      "Iteration  683 :   0.117422\n",
      "Iteration  684 :   0.111643\n",
      "Iteration  685 :   0.101879\n",
      "Iteration  686 :   0.114395\n",
      "Iteration  687 :   0.0732217\n",
      "Iteration  688 :   0.0441966\n",
      "Iteration  689 :   0.071363\n",
      "Iteration  690 :   0.0786219\n",
      "Iteration  691 :   0.142696\n",
      "Iteration  692 :   0.120557\n",
      "Iteration  693 :   0.101588\n",
      "Iteration  694 :   0.0242538\n",
      "Iteration  695 :   0.11791\n",
      "Iteration  696 :   0.217965\n",
      "Iteration  697 :   0.272999\n",
      "Iteration  698 :   0.0558839\n",
      "Iteration  699 :   0.104724\n",
      "Iteration  700 :   0.19272\n",
      "Iteration  701 :   0.255142\n",
      "Iteration  702 :   0.164408\n",
      "Iteration  703 :   0.156581\n",
      "Iteration  704 :   0.266768\n",
      "Iteration  705 :   0.169403\n",
      "Iteration  706 :   0.153605\n",
      "Iteration  707 :   0.0991483\n",
      "Iteration  708 :   0.0948462\n",
      "Iteration  709 :   0.11189\n",
      "Iteration  710 :   0.219451\n",
      "Iteration  711 :   0.0412364\n",
      "Iteration  712 :   0.346807\n",
      "Iteration  713 :   0.13361\n",
      "Iteration  714 :   0.128595\n",
      "Iteration  715 :   0.192626\n",
      "Iteration  716 :   0.284219\n",
      "Iteration  717 :   0.15393\n",
      "Iteration  718 :   0.13808\n",
      "Iteration  719 :   0.158609\n",
      "Iteration  720 :   0.0862746\n",
      "Iteration  721 :   0.208451\n",
      "Iteration  722 :   0.23731\n",
      "Iteration  723 :   0.108441\n",
      "Iteration  724 :   0.231347\n",
      "Iteration  725 :   0.215715\n",
      "Iteration  726 :   0.152721\n",
      "Iteration  727 :   0.205295\n",
      "Iteration  728 :   0.452811\n",
      "Iteration  729 :   0.330096\n",
      "Iteration  730 :   0.0929408\n",
      "Iteration  731 :   0.157125\n",
      "Iteration  732 :   0.130607\n",
      "Iteration  733 :   0.180265\n",
      "Iteration  734 :   0.0994616\n",
      "Iteration  735 :   0.166174\n",
      "Iteration  736 :   0.131813\n",
      "Iteration  737 :   0.0599146\n",
      "Iteration  738 :   0.251287\n",
      "Iteration  739 :   0.140536\n",
      "Iteration  740 :   0.28067\n",
      "Iteration  741 :   0.193222\n",
      "******************* End of an epoch ******************************\n",
      "******************* End of Training ******************************\n",
      "Wins:  16207.0\n",
      "Ties:  0.0\n",
      "losses:  4204.0\n",
      "Test Accuracy: 0.794032629465\n",
      "Test F1 Score: 0.794032629465\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_epochs = 1\n",
    "minibatch_size = 32\n",
    "m = 4#num_train\n",
    "p = 4#num_test\n",
    "\n",
    "wins_count = 0\n",
    "ties_count = 0\n",
    "losses_count = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        minibatches = mini_batches(X_train_1, X_train_0, minibatch_size)\n",
    "        #minibatches = mini_batches(X_dev_1, X_dev_0, m, minibatch_size)\n",
    "\n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "            #if i == 10:\n",
    "            #    break\n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            _ , temp_cost, pos, neg = sess.run([optimizer, cost, score_positive, score_negative], \n",
    "                        feed_dict={X_positive:minibatch_X_positive, \n",
    "                                X_negative:minibatch_X_negative})\n",
    "            \"\"\"\n",
    "            print(\"Epoch:\", epoch, \"Minibatch:\", i) \n",
    "            print(\"Positive score:\")\n",
    "            print(pos) \n",
    "            print(\"Negative score:\")\n",
    "            print(neg)\n",
    "            print(\"ranking loss:\", temp_cost)\n",
    "            \n",
    "            print(\"*************** End of a minibatch **********************************\")\n",
    "            \"\"\"\n",
    "            print(\"Iteration \",i, \":  \",temp_cost)\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        \n",
    "        #print(minibatch_cost)\n",
    "        print(\"******************* End of an epoch ******************************\")\n",
    "        print(\"******************* End of Training ******************************\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        minibatches = mini_batches(X_test_1, X_test_0, minibatch_size)\n",
    "        \n",
    "        wins = tf.greater(score_positive, score_negative)\n",
    "        number_wins = tf.reduce_sum(tf.cast(wins, tf.float32))\n",
    "        \n",
    "        ties = tf.equal(score_positive, score_negative)\n",
    "        number_ties = tf.reduce_sum(tf.cast(ties, tf.float32))\n",
    "\n",
    "        losses = tf.less(score_positive, score_negative)\n",
    "        number_losses = tf.reduce_sum(tf.cast(losses, tf.float32))\n",
    "        \n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "            \n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            num_wins, num_ties, num_losses = sess.run([number_wins, number_ties, number_losses], feed_dict={X_positive:minibatch_X_positive, X_negative:minibatch_X_negative})\n",
    "            \n",
    "            wins_count += num_wins\n",
    "            ties_count += num_ties\n",
    "            losses_count += num_losses\n",
    "        \n",
    "        \"\"\"\n",
    "        wins = tf.greater(score_positive, score_negative)\n",
    "        number_wins = tf.reduce_sum(tf.cast(wins, \"float\"))\n",
    "        \n",
    "        ties = tf.equal(score_positive, score_negative)\n",
    "        number_ties = tf.reduce_sum(tf.cast(ties, \"float\"))\n",
    "\n",
    "        losses = tf.less(score_positive, score_negative)\n",
    "        number_losses = tf.reduce_sum(tf.cast(losses, \"float\"))\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"Wins: \", wins_count)\n",
    "        print(\"Ties: \", ties_count)\n",
    "        print(\"losses: \", losses_count)\n",
    "        \n",
    "        recall = wins_count/(wins_count + ties_count + losses_count)\n",
    "        \n",
    "        precision = wins_count/(wins_count+losses_count)\n",
    "\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        accuracy = wins_count/(wins_count + ties_count + losses_count)\n",
    "        \n",
    "        \n",
    "        #test_accuracy, test_f1 = sess.run([accuracy, f1], feed_dict={X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "        #accuracy.eval(feed_dict={X_positive:X_test_1, X_negative:X_test_0})\n",
    "        #test_f1 = f1.eval({X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "        print(\"Test F1 Score:\", f1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
