{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utilities import my_callbacks\n",
    "from utilities import data_helper\n",
    "import optparse\n",
    "import sys\n",
    "import math\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: None\n",
      "{'X': 162058, '-': 2449404, 'O': 30440, 'S': 52415}\n",
      "Total vocabulary size in the whole dataset: 4\n",
      "['-', 'O', 'S', 'X', '0']\n"
     ]
    }
   ],
   "source": [
    "vocab = data_helper.load_all(filelist=\"final_data/wsj.all\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading entity-gird for pos and neg documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"loading entity-gird for pos and neg documents...\")\n",
    "\n",
    "X_train_1, X_train_0, E = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.train\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_dev_1, X_dev_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.dev\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n",
    "\n",
    "X_test_1, X_test_0, E    = data_helper.load_and_numberize_Egrid_with_Feats(\"final_data/wsj.test\", \n",
    "        perm_num = 20, maxlen=2000, window_size=6, E = E, vocab_list=vocab, emb_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................\n",
      "Num of traing pairs: 23744\n",
      "Num of dev pairs: 2678\n",
      "Num of test pairs: 20411\n",
      ".....................................\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_train_1)\n",
    "num_dev   = len(X_dev_1)\n",
    "num_test  = len(X_test_1)\n",
    "#assign Y value\n",
    "y_train_1 = [1] * num_train \n",
    "y_dev_1 = [1] * num_dev \n",
    "y_test_1 = [1] * num_test \n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of dev pairs: \" + str(num_dev))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "#print(\"Num of permutation in train: \" + str(opts.p_num)) \n",
    "#print(\"The maximum in length for CNN: \" + str(opts.maxlen))\n",
    "print('.....................................')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomly shuffle the training data\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_1)\n",
    "np.random.seed(113)\n",
    "np.random.shuffle(X_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X_positive, X_negative, vocab, E, print_ = False):\n",
    "    \"\"\"\n",
    "    Implements forward propagation of Neural coherence model\n",
    "    \n",
    "    Arguments:\n",
    "    X_positive -- A Placeholder for positive document\n",
    "    X_negative -- A Placeholder for negative document\n",
    "    vocab -- Vocabulary list of entire entity grid list\n",
    "    E -- initialized values for embedding matrix\n",
    "    print_ -- Whether size of the variables to be printed\n",
    "    \n",
    "    Returns: \n",
    "    out_positive -- Coherence Score for positive document\n",
    "    out_negative -- Coherence Score for negative document\n",
    "    parameters -- a dictionary of tensors containing trainable parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Placeholders\n",
    "    #X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    #X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #opts.maxlen=2000\n",
    "    \n",
    "\n",
    "    ## First Layer of NN: Transform each grammatical role in the grid into distributed representation - a real valued vector\n",
    "    \n",
    "    \n",
    "    #Shared embedding matrix\n",
    "    #W_embedding = tf.get_variable(\"W_embedding\", [len(vocab), 100], initializer = tf.contrib.layers.xavier_initializer()) #embedding matrix \n",
    "    #E = np.float32(E) # DataType of E is float64, which is not in list of allowed values in conv1D. Allowed DataType: float16, float32\n",
    "    E =  tf.convert_to_tensor(E, tf.float32) \n",
    "    W_embedding = tf.get_variable(\"W_embedding\", initializer = E) #embedding matrix \n",
    "   \n",
    "    \n",
    "    #Look up layer\n",
    "    \n",
    "    #for positive document\n",
    "    embedding_positive = tf.nn.embedding_lookup(W_embedding, X_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    embedding_negative = tf.nn.embedding_lookup(W_embedding, X_negative)\n",
    "\n",
    "\n",
    "    ## Second Layer of NN: Convolution Layer\n",
    "    \n",
    "    \n",
    "    #shared filter and bias\n",
    "    w_size = 6       #filter_size\n",
    "    emb_size = 100   #embedding_size \n",
    "    nb_filter = 150  #num_filters \n",
    "\n",
    "    filter_shape = [w_size, emb_size, nb_filter]\n",
    "\n",
    "    #W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 0)) #filter for covolution layer 1\n",
    "    W_conv_layer_1 = tf.get_variable(\"W_conv_layer_1\", shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer(seed = 2018)) #filter for covolution layer 1\n",
    "    b_conv_layer_1 =  tf.get_variable(\"b_conv_layer_1\", shape=[nb_filter], initializer = tf.constant_initializer(0.0))  #bias for convolution layer 1\n",
    "\n",
    "    \n",
    "       \n",
    "    #1D Convolution for positive document\n",
    "    conv_layer_1_positive = tf.nn.conv1d(embedding_positive, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_positive = tf.nn.bias_add(conv_layer_1_positive, b_conv_layer_1)    \n",
    "    h_conv_layer_1_positive = tf.nn.relu(conv_layer_1_with_bias_positive, name=\"relu_conv_layer_1_positive\") # Apply nonlinearity\n",
    "    \n",
    "    \n",
    "    #1D Convolution for negative document\n",
    "    conv_layer_1_negative = tf.nn.conv1d(embedding_negative, W_conv_layer_1, stride=1, padding=\"VALID\")  #embedding and W_conv_layer_1 both are 3D matrix\n",
    "    conv_layer_1_with_bias_negative = tf.nn.bias_add(conv_layer_1_negative, b_conv_layer_1)    \n",
    "    h_conv_layer_1_negative = tf.nn.relu(conv_layer_1_with_bias_negative, name=\"relu_conv_layer_1_negative\") # Apply nonlinearity\n",
    "\n",
    "    \n",
    "\n",
    "    ## Third Layer of NN: Pooling Layer\n",
    "    \n",
    "    \n",
    "    #1D Pooling for positive document\n",
    "    m_layer_1_positive = tf.nn.pool(h_conv_layer_1_positive, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "    #1D Pooling for negative document\n",
    "    m_layer_1_negative = tf.nn.pool(h_conv_layer_1_negative, window_shape = [6], strides = [6], pooling_type = 'MAX', padding=\"VALID\")\n",
    "\n",
    "\n",
    "\n",
    "    ## Fourth Layer of NN: Fully Connected Layer\n",
    "    \n",
    "    #Dropout Early [As Dat Used]\n",
    "    \n",
    "    #for positive document\n",
    "    #drop_out_early_positive = tf.nn.dropout(m_layer_1_positive, keep_prob=0.5)\n",
    "    \n",
    "    #for negative document\n",
    "    #drop_out_early_negative = tf.nn.dropout(m_layer_1_negative, keep_prob=0.5)\n",
    "    \n",
    "    \n",
    "    #Flatten\n",
    "    \n",
    "    #for positive document\n",
    "    flatten_positive = tf.contrib.layers.flatten(m_layer_1_positive)\n",
    "    #flatten_positive = tf.contrib.layers.flatten(drop_out_early_positive)\n",
    "    \n",
    "    #for negative document\n",
    "    flatten_negative = tf.contrib.layers.flatten(m_layer_1_negative)\n",
    "    #flatten_negative = tf.contrib.layers.flatten(drop_out_early_negative)\n",
    "    \n",
    "\n",
    "    #Dropout\n",
    "    \n",
    "    #for positive document\n",
    "    drop_out_positive = tf.nn.dropout(flatten_positive, keep_prob=0.5, seed=2018)\n",
    "    \n",
    "    #for negative document\n",
    "    drop_out_negative = tf.nn.dropout(flatten_negative, keep_prob=0.5, seed=2018)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Coherence Scoring\n",
    "    v_fc_layer = tf.get_variable(\"v_fc_layer\", shape = [49800, 1], initializer = tf.contrib.layers.xavier_initializer(seed = 2018)) #Weight matrix for final layer\n",
    "    b_fc_layer =  tf.get_variable(\"b_fc_layer\", shape=[1], initializer = tf.constant_initializer(0.0))  #bias for final layer\n",
    "\n",
    "    \n",
    "    \n",
    "    #for positive document\n",
    "    #out_positive = tf.contrib.layers.fully_connected(drop_out_positive, num_outputs = 1, activation_fn=None)\n",
    "    #out_positive = tf.sigmoid(out_positive)\n",
    "    out_positive = tf.add(tf.matmul(drop_out_positive, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    #for negative document\n",
    "    #out_negative = tf.contrib.layers.fully_connected(drop_out_negative, num_outputs = 1, activation_fn=None)\n",
    "    #out_negative = tf.sigmoid(out_negative)\n",
    "    out_negative = tf.add(tf.matmul(drop_out_negative, v_fc_layer), b_fc_layer)\n",
    "    \n",
    "    \n",
    "    parameters = {\"W_embedding\": W_embedding,\n",
    "                  \"W_conv_layer_1\": W_conv_layer_1,\n",
    "                  \"b_conv_layer_1\": b_conv_layer_1,\n",
    "                  \"v_fc_layer\": v_fc_layer,\n",
    "                  \"b_fc_layer\": b_fc_layer}\n",
    "    \n",
    "    \n",
    "    if(print_):\n",
    "        print(\"Layer (type)          Output Shape\")\n",
    "        print(\"_________________________________________\")\n",
    "        print(\"\\nInputLayer:\")\n",
    "        print(\"X_positive           \",   X_positive.shape)\n",
    "        print(\"X_negative           \",   X_negative.shape)\n",
    "        print(\"\\nEmbedding Layer:\")\n",
    "        print(\"Embedding Matrix     \",   W_embedding.shape)\n",
    "        print(\"Embedding Positive   \",   embedding_positive.shape)\n",
    "        print(\"Embedding Negative   \",   embedding_negative.shape)\n",
    "        print(\"\\nConvolution 1D Layer:\")\n",
    "        print(\"Filter Shape         \",   W_conv_layer_1.shape)\n",
    "        print(\"Conv Positive        \",   h_conv_layer_1_positive.shape)\n",
    "        print(\"Conv Negative        \",   h_conv_layer_1_negative.shape)\n",
    "        print(\"\\nMax Pooling 1D Layer:\")\n",
    "        print(\"MaxPool Positive     \",   m_layer_1_positive.shape)\n",
    "        print(\"MaxPool Negative     \",   m_layer_1_negative.shape)\n",
    "        print(\"\\nFlatten Layer: \")\n",
    "        print(\"Flatten Positive     \",   flatten_positive.shape)\n",
    "        print(\"Flatten Negative     \",   flatten_negative.shape)\n",
    "        print(\"\\nDropout Layer: \")\n",
    "        print(\"Dropout Positive     \",   drop_out_positive.shape)\n",
    "        print(\"Dropout Negative     \",   drop_out_negative.shape)\n",
    "        print(\"\\nFully Connected Layer:\")\n",
    "        print(\"FC Positive          \",   out_positive.shape)\n",
    "        print(\"FC Negative          \",   out_negative.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return out_positive, out_negative, parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ranking_loss(pos, neg):\n",
    "    \"\"\"\n",
    "    Implements the ranking objective.\n",
    "    \n",
    "    Arguments:\n",
    "    pos -- score for positive document batch\n",
    "    neg -- score for negative document batch\n",
    "    \n",
    "    Returns:\n",
    "    Average ranking loss for the batch  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    loss = tf.maximum(1.0 + neg - pos, 0.0) \n",
    "    #print(loss)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mini_batches(X, Y, mini_batch_size = 32):\n",
    "    \"\"\"\n",
    "    Creates minibatches.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Positive Documents\n",
    "    Y -- Negative Documents\n",
    "    mini_batch_size -- Size of each mini batch. \n",
    "    \n",
    "    Returns:\n",
    "    list of mini batches from the positive and negative documents.\n",
    "    \n",
    "    \"\"\"\n",
    "    m = len(X)   \n",
    "    mini_batches = []\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) \n",
    "    \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer (type)          Output Shape\n",
      "_________________________________________\n",
      "\n",
      "InputLayer:\n",
      "X_positive            (?, 2000)\n",
      "X_negative            (?, 2000)\n",
      "\n",
      "Embedding Layer:\n",
      "Embedding Matrix      (5, 100)\n",
      "Embedding Positive    (?, 2000, 100)\n",
      "Embedding Negative    (?, 2000, 100)\n",
      "\n",
      "Convolution 1D Layer:\n",
      "Filter Shape          (6, 100, 150)\n",
      "Conv Positive         (?, 1995, 150)\n",
      "Conv Negative         (?, 1995, 150)\n",
      "\n",
      "Max Pooling 1D Layer:\n",
      "MaxPool Positive      (?, 332, 150)\n",
      "MaxPool Negative      (?, 332, 150)\n",
      "\n",
      "Flatten Layer: \n",
      "Flatten Positive      (?, 49800)\n",
      "Flatten Negative      (?, 49800)\n",
      "\n",
      "Dropout Layer: \n",
      "Dropout Positive      (?, 49800)\n",
      "Dropout Negative      (?, 49800)\n",
      "\n",
      "Fully Connected Layer:\n",
      "FC Positive           (?, 1)\n",
      "FC Negative           (?, 1)\n"
     ]
    }
   ],
   "source": [
    "## Create Placeholders\n",
    "X_positive = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for positive document\n",
    "X_negative = tf.placeholder(tf.int32, shape = [None, 2000]) #Placeholder for negative document\n",
    "\n",
    "# Forward propagation\n",
    "score_positive, score_negative, parameters = forward_propagation(X_positive, X_negative, vocab, E, print_=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost function:\n",
    "cost = ranking_loss(score_positive, score_negative)\n",
    "\n",
    "# Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001, decay=0.0, momentum=0.9, epsilon=1e-8).minimize(cost)\n",
    "\n",
    "\n",
    "## Using keras RMSProp\n",
    "\n",
    "W_embedding = parameters[\"W_embedding\"]\n",
    "W_conv_layer_1 = parameters[\"W_conv_layer_1\"]\n",
    "b_conv_layer_1 = parameters[\"b_conv_layer_1\"]\n",
    "v_fc_layer = parameters[\"v_fc_layer\"]\n",
    "b_fc_layer = parameters[\"b_fc_layer\"]\n",
    "optimizer = tf.keras.optimizers.RMSprop().get_updates(cost, [W_embedding, W_conv_layer_1, b_conv_layer_1, v_fc_layer, b_fc_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_1 = X_train_1[:100, :]\\nX_train_0 = X_train_0[:100, :]\\nX_test_1 = X_test_1[:100, :]\\nX_test_0 = X_test_0[:100, :]\\n\\n\\nnum_train = len(X_train_1)\\nnum_test  = len(X_test_1)\\n\\n\\nprint(\\'.....................................\\')\\nprint(\"Num of traing pairs: \" + str(num_train))\\nprint(\"Num of test pairs: \" + str(num_test))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_train_1 = X_train_1[:100, :]\n",
    "X_train_0 = X_train_0[:100, :]\n",
    "X_test_1 = X_test_1[:100, :]\n",
    "X_test_0 = X_test_0[:100, :]\n",
    "\n",
    "\n",
    "num_train = len(X_train_1)\n",
    "num_test  = len(X_test_1)\n",
    "\n",
    "\n",
    "print('.....................................')\n",
    "print(\"Num of traing pairs: \" + str(num_train))\n",
    "print(\"Num of test pairs: \" + str(num_test))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Epoch:  0   ******************\n",
      "Wins:  16288\n",
      "Ties:  0\n",
      "losses:  4123\n",
      "Test Accuracy: 0.79800107785\n",
      "Test F1 Score: 0.79800107785\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  1   ******************\n",
      "Wins:  16136\n",
      "Ties:  0\n",
      "losses:  4275\n",
      "Test Accuracy: 0.790554112978\n",
      "Test F1 Score: 0.790554112978\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  2   ******************\n",
      "Wins:  16236\n",
      "Ties:  0\n",
      "losses:  4175\n",
      "Test Accuracy: 0.795453431973\n",
      "Test F1 Score: 0.795453431973\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  3   ******************\n",
      "Wins:  16188\n",
      "Ties:  0\n",
      "losses:  4223\n",
      "Test Accuracy: 0.793101758856\n",
      "Test F1 Score: 0.793101758856\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  4   ******************\n",
      "Wins:  16257\n",
      "Ties:  0\n",
      "losses:  4154\n",
      "Test Accuracy: 0.796482288962\n",
      "Test F1 Score: 0.796482288962\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  5   ******************\n",
      "Wins:  15942\n",
      "Ties:  0\n",
      "losses:  4469\n",
      "Test Accuracy: 0.781049434129\n",
      "Test F1 Score: 0.781049434129\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  6   ******************\n",
      "Wins:  16214\n",
      "Ties:  0\n",
      "losses:  4197\n",
      "Test Accuracy: 0.794375581794\n",
      "Test F1 Score: 0.794375581794\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  7   ******************\n",
      "Wins:  16131\n",
      "Ties:  0\n",
      "losses:  4280\n",
      "Test Accuracy: 0.790309147029\n",
      "Test F1 Score: 0.790309147029\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  8   ******************\n",
      "Wins:  16018\n",
      "Ties:  0\n",
      "losses:  4393\n",
      "Test Accuracy: 0.784772916565\n",
      "Test F1 Score: 0.784772916565\n",
      "\n",
      "\n",
      "\n",
      "***********Epoch:  9   ******************\n",
      "Wins:  16090\n",
      "Ties:  0\n",
      "losses:  4321\n",
      "Test Accuracy: 0.788300426241\n",
      "Test F1 Score: 0.788300426241\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "num_epochs = 10\n",
    "minibatch_size = 32\n",
    "m = num_train\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        minibatches = mini_batches(X_train_1, X_train_0, minibatch_size)\n",
    "        #minibatches = mini_batches(X_dev_1, X_dev_0, m, minibatch_size)\n",
    "\n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "            #if i == 10:\n",
    "            #    break\n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            _ , temp_cost, pos, neg = sess.run([optimizer, cost, score_positive, score_negative], \n",
    "                        feed_dict={X_positive:minibatch_X_positive, \n",
    "                                X_negative:minibatch_X_negative})\n",
    "            \"\"\"\n",
    "            print(\"Epoch:\", epoch, \"Minibatch:\", i) \n",
    "            print(\"Positive score:\")\n",
    "            print(pos) \n",
    "            print(\"Negative score:\")\n",
    "            print(neg)\n",
    "            print(\"ranking loss:\", temp_cost)\n",
    "            \n",
    "            print(\"*************** End of a minibatch **********************************\")\n",
    "            \"\"\"\n",
    "            #print(\"Iteration \",i, \":  \",temp_cost)\n",
    "            minibatch_cost += temp_cost / num_minibatches\n",
    "        \n",
    "        #print(minibatch_cost)\n",
    "        #print(\"******************* End of an epoch ******************************\")\n",
    "        #print(\"******************* End of Training ******************************\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        wins_count = 0\n",
    "        ties_count = 0\n",
    "        losses_count = 0\n",
    "        \n",
    "        minibatches = mini_batches(X_test_1, X_test_0, minibatch_size)\n",
    "        \n",
    "        wins = tf.greater(score_positive, score_negative)\n",
    "        number_wins = tf.reduce_sum(tf.cast(wins, tf.int32))\n",
    "        \n",
    "        ties = tf.equal(score_positive, score_negative)\n",
    "        number_ties = tf.reduce_sum(tf.cast(ties, tf.int32))\n",
    "\n",
    "        losses = tf.less(score_positive, score_negative)\n",
    "        number_losses = tf.reduce_sum(tf.cast(losses, tf.int32))\n",
    "        \n",
    "        for (i, minibatch) in enumerate(minibatches):\n",
    "            \n",
    "            (minibatch_X_positive, minibatch_X_negative) = minibatch\n",
    "            \n",
    "            num_wins, num_ties, num_losses = sess.run([number_wins, number_ties, number_losses], feed_dict={X_positive:minibatch_X_positive, X_negative:minibatch_X_negative})\n",
    "            \n",
    "            wins_count += num_wins\n",
    "            ties_count += num_ties\n",
    "            losses_count += num_losses\n",
    "        \n",
    "        \n",
    "        \n",
    "        recall = wins_count/(wins_count + ties_count + losses_count)\n",
    "        \n",
    "        precision = wins_count/(wins_count+losses_count)\n",
    "\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        accuracy = wins_count/(wins_count + ties_count + losses_count)\n",
    "        \n",
    "        \n",
    "        #test_accuracy, test_f1 = sess.run([accuracy, f1], feed_dict={X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "        #accuracy.eval(feed_dict={X_positive:X_test_1, X_negative:X_test_0})\n",
    "        #test_f1 = f1.eval({X_positive:X_test_1, X_negative:X_test_0})\n",
    "        \n",
    "        print(\"***********Epoch: \",epoch,\"  ******************\")\n",
    "        \n",
    "        print(\"Wins: \", wins_count)\n",
    "        print(\"Ties: \", ties_count)\n",
    "        print(\"losses: \", losses_count)\n",
    "        \n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "        print(\"Test F1 Score:\", f1)\n",
    "        print(\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
